---
title: "Predict Car Price With Lasso Regression"
author: "Summer Wang, Yawen Ye, Matt Staton, Wenduo Wang"
date: "July 22, 2016"
output: html_document
---
# 1. Introduction
Lasso regression is a powerful tool to generate predictive models based on continuous and categorical variables. With cross validation, this method improves bias-variance balance of traditional linear regression methods by penalizing weights on minor predictors. Therefore, when working with out-of-sample data, the model is less affected by noise and extrapolates easily.  
We will apply this method to create a prediction model on car's price, based on the `Cars.csv` data. The goal is to find a model that minimizes out-of-sample error, which is defined as the Root Mean Square Error between predicted $\hat{Price}$ and actual $Price$.
It should be mentioned here, however, that we have also tried Ridge regression in the same fashion, but due to the out-of-sample RMSE comparison, which is not shown here for computational reasons, Lasso regression is chosen for achieving better fit.

* You are welcome to implement Ridge regression as well. All you need to do are:
    + Replace all "lasso" with "ridge" in the following scripts, and
    + set `alpha=0` in the `cv.glmnet` function

> __WARNING: before running the following code, please make sure local work is saved and enough CPU & RAM is available for the computation.__  

# 2. Result and Discussion
To initialize modeling, first release existent variables to free memory. This is important because the following matrix operations are computationally intensive.
```{r, message=FALSE}
rm(list=ls())
gc()
```
Then include essential libraries for data frame, matrix and Lasso regression functions, which in this case are `dplyr` and `glmnet`.
`knitr` and `formatR` libraries are required for formatting the R Markdown document.
`compiler` library offers performance boost by setting `enableJIT(3)`, which is a quasi-pre-compiling function speeding up `for` loops.
```{r, message=FALSE}
library(Hmisc)
library(MASS)
library(VIM)
library(caret)
library(mboost)
library(gbm)
library(dplyr)
library(glmnet)
library(knitr)
library(formatR)
library(compiler)
enableJIT(3)
```
The method described below can be summarised as:  

* Read in training data and convert categorical variables into dummy variables with logical values of `TRUE` or `FAlSE`, and thus removing `NA's`
* Scale `mileage` and `featureCount` to bring all variables to a comparable level
* Create interaction terms between different (dummy) variables
* Generate a Lasso regression model on the transformed variables, including interaction terms, with k-fold cross validation
* Read in test data and convert the categorical variables as mentioned above, with the same scaling procedure
* Also create interaction terms
* Use the Lasso regression model to calculate $\hat{Price}$, and take its RMSE against the actual $Price$

## 2.1. The following snippets define several functions to transform the data and implement Lasso regression.  
Define a function `unfold_name` that takes a column variable and a string. This function performs string formatting and returns a new vector of variable names. For example, when passed the `color` column and string `color` to the function, a new vector will be returned such as `c("color: red", "color: blue", ...)`. For columns containing `NA` values, such value will be recorded as `unknown`. In the case of `color`, there will be a string `..."color: unknown"...` in the returned vector.
This function prepares
```{r, messgae=FALSE, tidy=TRUE}
unfold_name <- function(variable, var_name=c("")){
    subset <- as.character(unique(variable))
    
    for (i in c(1:length(subset))){
        if (is.na(subset[i])){
            subset[i] <- paste(as.character(var_name), ": unknown", sep="")
        }
        
        else {
            subset[i] <- paste(as.character(var_name), ": ", subset[i], sep="")
        }
    }
    
    subset <- sort(subset)
    
    return(subset)
}
```
The `matrix_product` function does matrix multiplication between given columns. The arguments are the matrix, the column numbers of base variables and those of multiple variables. It is used to obtain interaction terms between dummy variables.
Example: `matrix_product(cars_data, col_num_year, col_num_trim)` will multiply `cars_data[, col_num_year]` as a whole by each column in `cars_data[, col_num_trim]`, and `cbind` resultant matrices.
```{r, message=FALSE, tidy=TRUE}
matrix_product <- function(A, col1, col2){
    B <- matrix(rep(0, nrow(A)), nrow=nrow(A))
    for (i in col2){
        B <- cbind(B, A[, col1]*A[, i])
    }
    return(B[, -1])
}
```
Since Lasso regression depends on simultaneously _penalising_ all predictors to reduce the contribution of less important ones (_noise_), it is more effective working with variables of comparable values. In `Cars.csv` data, among the predictors, `mileage` and `featureCount` are largest in absolute value and need to be brought down to an order of 1 - 10. To achieve this, we have tried different ways and decided on taking $\log(mileage)$ and $\sqrt{featureCount}/5$.

> The difference between scaling methods could be better understood with a graphical illustration, but due to the implied intense calculation for a desktop, we have omitted this part. Yet it could be easily plotted by iterating a series of scaling functions, given a fast computer and, more importantly, __patience__.

```{r, message=FALSE, tidy=TRUE}
scaling_data <- function(original_matrix){
    scaled_matrix <- original_matrix
    scaled_matrix[, "mileage"] <- log(as.numeric(original_matrix[, "mileage"]))
    scaled_matrix[, "featureCount"] <- as.numeric(original_matrix[, "featureCount"])^.5/5
    return(scaled_matrix)
}
```
The following `data_pruning` function is probably the __core__ procedure in the process of transforming the raw data into a well-structured one for Lasso regression.
The `new_data` read from `Cars.csv` file is fed into the `data_pruning` function, and for each categorical variable, it will be "unfolded" into dummy variables of the length of its unique values, with `NA's` occupying a separate column (though the `NA's` can be reasonably discarded as well, we prefer to keep them and take advantage of this information).
Secondly, the values in `new_data` are projected into the extended data frame. While numerical variables including `mileage` are intact, categorical variables are now represented as a data frame of logical vectors. For example,
```{r, echo=FALSE}
Example <- matrix(c("green", "red", "yellow", "...", "..."), nrow=5)
colnames(Example) <- "color"
kable(Example, format="markdown")
```

now becomes

```{r, echo=FALSE}
Example <- matrix(c(TRUE, FALSE, FALSE, "...", "...",
                    FALSE, TRUE, FALSE, "...", "...",
                    FALSE, FALSE, TRUE, "...", "...",
                    "...", "...", "...", "...", "..."),
                  nrow=5)
colnames(Example) <- c("color: green", "color: red", "color: yellow", "...")
kable(Example, format="markdown")
```

By this point, we have obtained a bigger data frame with all dummy variables derived from the original categorical variables. Next we need to 1. convert the data frame to matrix for ease of use as required by many functions, and 2. scale `mileage` and `featureCount` as explained above.
For 1, `model.matrix` function is used, which will convert the data frame into a matrix, and remove the `X` column. But by design, this function will also add an `Intercept` column into the new matrix. After this conversion, the matrix is now all of numeric values, with `TRUE` and `FALSE` translated into `1` and `0` respectively. Yet a side-effect is that the column names have also been changed for those previously logical columns, and again, for convenience in the future, an assignment operation is done again to restore the normal column names.
For 2, `scaling_data` function is called. The process is very straight forward, and thus not repeated here.

```{r, message=FALSE, tidy=TRUE}
data_pruning <- function(new_data){
    
    #create an empty vector to store the unfolded feature names
    feature_names <- c()
    
    #store the original variable names
    original_names <- colnames(new_data)
    
    #store the names of numerical variables
    numeric_vars <- c("X", "mileage", "featureCount", "price")
    
    #unfolded categorical variables will be expanded horizontally
    for (i in c(1:ncol(new_data))){
        if (original_names[i] %in% numeric_vars){
            feature_name <- original_names[i]
        }
        
        else {
            feature_name <- unfold_name(new_data[, i], original_names[i])
        }
        
        feature_names <- c(feature_names, feature_name)
    }

    varnames_logic <- feature_names[!(feature_names %in% numeric_vars)]
    varnames_logic <- gsub("^.+: ", "", varnames_logic)
    varnames_logic[endsWith(varnames_logic, "unknown")] <- NA
    new_dataset <- data.frame(new_data[, 1])
    k <- 0
    
    for (i in c(1:dim(new_data)[2])){
        if (colnames(new_data)[i] %in% numeric_vars){
            new_dataset <- cbind(new_dataset, new_data[, i])
        } else {
            for (j in c(1:length(unique(new_data[,i])))){
                pending_col <- new_data[, i]
                if (is.na(varnames_logic[j+k])){
                    pending_col[!is.na(pending_col)] <- FALSE
                    pending_col[is.na(pending_col)] <- TRUE
                } else {
                    pending_col[is.na(pending_col)] <- FALSE
                    pending_col <- (pending_col == varnames_logic[j+k])
                }
                new_dataset <- cbind(new_dataset, pending_col)
            }
            k <- k + j
        }
    }

    clean_dataset <- new_dataset[, 2:ncol(new_dataset)]
    
    colnames(clean_dataset) <- feature_names
    
    rm(new_dataset)
    gc()
    
    data_matrix <- model.matrix(~., data=clean_dataset[, -1])
    
    rm(clean_dataset)
    gc()
    
    for (i in c(1:ncol(data_matrix))){
        colnames(data_matrix)[i] <- gsub("`|TRUE", "", colnames(data_matrix)[i])
    }
    
    lasso_matrix <- scaling_data(data_matrix)
    
    return(lasso_matrix)
}
```

Now we have come to actually fitting the data using Lasso regression. But to complete this, there is still some way to go.
First, we used __IBM Watson__ to shed a light on the "drivers" of `price`, mainly because of its relatively fast speed and excellent visualization features. The major drivers of "strength" over 80% are:
```{r, echo=FALSE, tidy=TRUE}
Example <- matrix(c(
                    "year",
                    "trim",
                    "mileage",
                    "mileage",
                    "condition",
                    "condition",
                    "mileage",
                    "year",
                    "isOneOwner",
                    "year",
                    "year",
                    "year",
                    "year",
                    "year",
                    "year",
                    "condition",
                    "mileage",
                    "mileage",
                    "mileage",
                    "mileage",
                    "mileage",
                    "mileage",
                    "mileage",
                    "mileage",
                    "mileage",
                    "displacement",
                    "year",
                    "displacement",
                    "trim",
                    "displacement",
                    "year",
                    "year",
                    "wheelSize",
                    "year",
                    "color",
                    "featureCount",
                    "region",
                    "soundSystem",
                    "wheelType",
                    "",
                    "trim",
                    "condition",
                    "isOneOwner",
                    "soundSystem",
                    "wheelSize",
                    "color",
                    "featureCount",
                    "region",
                    "wheelType",
                    "",
                    "96%",
                    "95%",
                    "92%",
                    "92%",
                    "91%",
                    "90%",
                    "90%",
                    "90%",
                    "89%",
                    "89%",
                    "89%",
                    "89%",
                    "89%",
                    "89%",
                    "89%",
                    "88%",
                    "86%",
                    "85%",
                    "85%",
                    "85%",
                    "84%",
                    "84%",
                    "84%",
                    "84%",
                    "84%"),
                    ncol=3)
colnames(Example) <- c("Predictor 1", "Predictor 2", "Strength")
kable(Example, format="markdown")
```

Another way to accomplish this exploratory task is create all possible interaction terms between different dummy variables, and fit a linear model on them to compare the coefficients confidence intervals. But this comes with a very expensive cost of computation and time, due to the sheer size
of the combination of products of two different matrices. So a decision is made that we will accept __Watson's__ perspective and create interaction terms in the aforementioned table.
With the original stand-alone variables and generated interaction terms, now it is time to pass the values to `cv.glmnet` function to establish a model. `cv.glmnet`, as indicated by its name, does k-fold cross validation internally to conclude on a single best-fit model. In the process, the in-sample RMSE is printed, and finally the model is returned.
The above interaction terms take a big chunk of memory, so it is a good practice to `rm` them and use `gc` to release the memory.
```{r, message=FALSE, tidy=TRUE}
lasso_fit <- function(lasso_matrix){
    
    displacement_col <- 61:75
    year_col <- 23:45
    trim_col <- 2:14
    mileage_col <- 22
    condition_col <- 17:19
    wheelSize_col <- 154:161
    isOneOwner_col <- 20:21
    color_col <- 46:60
    featureCount_col <- 162
    region_col <- 132:141
    soundSystem_col <- 142:148
    wheelType_col <- 149:153
    
    interact_terms <- cbind(
        displacement_year <- matrix_product(lasso_matrix, displacement_col, year_col),
        year_trim <- matrix_product(lasso_matrix, year_col, trim_col),
        displacement_mileage <- matrix_product(lasso_matrix, displacement_col, mileage_col),
        trim_mileage <- matrix_product(lasso_matrix, trim_col, mileage_col),
        displacement_condition <- matrix_product(lasso_matrix, displacement_col, condition_col),
        year_condition <- matrix_product(lasso_matrix, year_col, condition_col),
        year_mileage <- matrix_product(lasso_matrix, year_col, mileage_col),
        wheelSize_year <- matrix_product(lasso_matrix, wheelSize_col, year_col),
        year_isOneOwner <- matrix_product(lasso_matrix, year_col, isOneOwner_col),
        color_year <- matrix_product(lasso_matrix, color_col, year_col),
        featureCount_year <- matrix_product(lasso_matrix, featureCount_col, year_col),
        region_year <- matrix_product(lasso_matrix, region_col, year_col),
        soundSystem_year <- matrix_product(lasso_matrix, soundSystem_col, year_col),
        wheelType_year <- matrix_product(lasso_matrix, wheelType_col, year_col),
        trim_condition <- matrix_product(lasso_matrix, trim_col, condition_col),
        condition_mileage <- matrix_product(lasso_matrix, condition_col, mileage_col),
        isOneOwner_mileage <- matrix_product(lasso_matrix, isOneOwner_col, mileage_col),
        soundSystem_mileage <- matrix_product(lasso_matrix, soundSystem_col, mileage_col),
        wheelSize_mileage <- matrix_product(lasso_matrix, wheelSize_col, mileage_col),
        color_mileage <- matrix_product(lasso_matrix, color_col, mileage_col),
        featureCount_mileage <- matrix_product(lasso_matrix, featureCount_col, mileage_col),
        region_mileage <- matrix_product(lasso_matrix, region_col, mileage_col),
        wheelType_mileage <- matrix_product(lasso_matrix, wheelType_col, mileage_col)
    )
    
    # use Lasso regression to generate a linear model including interaction terms
    lasso_model <- cv.glmnet(y=lasso_matrix[, "price"], x=cbind(
        lasso_matrix[, -c(1, 163)],
        interact_terms),
        nfolds=10,
        alpha=1
        )
    
    RMSE_in <- mean((predict(lasso_model, cbind(
        training_set_4lasso[, -c(1, 163)],
        interact_terms)) - training_set_4lasso[, "price"])^2)^.5
    
    cat("In sample RMSE:", RMSE_in)
    
    # clean up variables to free memory
    rm(interact_terms)
    rm(RMSE_in)
    gc()

    return(lasso_model)
}
```

Next we need to consider testing our model with out-of-sample data. Similar to the training data, the test data is stored in the original variables some of which are categorical, so therefore we still need to transform the test data into dummy variables stored in a matrix.
But the procedures to prune the test data are a bit different. At this point it is not necessary to create a matrix bespoke to the test data, because we already have the structure of training data (the matrix used to train the regression model), and that is exactly what is needed to testing, otherwise our model will not recognize the data. So in the following function, we basically need to convert the test data into the format of above training data.

1. Pass the `test data`, `training_data` (read from `csv`) and a vector of column names to the `test_data_pruning` function
2. The function will create an empty matrix `test_data_matrix` using the provided column names
3. It then compares all the values of each column in `test_data` with the dummy variable names (i.e. column names) of the new empty matrix. Here heed is taken of the potential non-match. For example, in our `test data`, there might be a new `color` _XXX_ which is not included in our training data, and in this case, the `test_data_pruning` function will categorize _XXX_ as _unknonw_ and assign `1` to the corresponding cell in the `color: unknown` column.
3. Scaling is performed on the new matrix.
Then the matrix is returned, tailored for prediction with our model.
```{r, message=FALSE, tidy=TRUE}
test_data_pruning <- function(test_data, training_data, varnames){
    test_set_matrix <- matrix(0, nrow=nrow(test_data), ncol=length(varnames))
    colnames(test_set_matrix) <- varnames
    
    # test_data[is.na(test_data)] <- "unknown"
    test_data_matrix <- as.matrix(test_data)
    
    #store the names of numerical variables
    numeric_vars <- c("mileage", "featureCount", "price")

    for (i in c(1:ncol(test_data))){
        if (colnames(test_data)[i] %in% numeric_vars){
            test_set_matrix[, colnames(test_data)[i]] <-  as.numeric(test_data_matrix[, i])
            next
        }
        
        test_data[is.na(test_data[, i]), i] <- "unknown"
        
        if (paste(colnames(test_data)[i], ": unknown") %in% varnames){
            mask <- test_data_matrix[, i] %in% unique(training_data[, colnames(test_data)[i]])
            test_set_matrix[!mask, paste(colnames(test_data)[i], ": unknown")] <- 1
            next
        }
        for (j in c(1:length(varnames))){
            if (startsWith(varnames[j], colnames(test_data)[i])){
                pattern <- gsub("^.+: ", "", varnames[j])
                row_mask <- which(test_data_matrix[, i] == pattern)
                test_set_matrix[row_mask, j] <- 1
            }
        }
    }
    
    colnames(test_set_matrix) <- varnames
    test_set_matrix <- scaling_data(test_set_matrix)

    return(test_set_matrix)
}
```

The following function is the `lasso_prediction_RMSE` function, whose name says it performs prediction and returns RMSE. The prediction process is very much the same with training the regression model, except it is much faster.
The function takes two arguments: the Lasso regression model, and re-formatted test data. To ensure the re-formatted test data are numerical, an extra conversion is in place. Then the interaction terms are created according to the `lasso_fit` function, and passed along with the test data into the `predict` function, thereby the out-of-sample `RMSE` is calculated and returned.
Note: before return `RMSE`, the interaction terms are removed and memory is released back to the system.
```{r, message=FALSE, tidy=TRUE}
lasso_prediction_RMSE <- function(lasso_model, lasso_set){
    lasso_matrix <- matrix(as.numeric(lasso_set), ncol=ncol(lasso_set))
    
    colnames(lasso_matrix) <- colnames(lasso_set)
    
    displacement_col <- 61:75
    year_col <- 23:45
    trim_col <- 2:14
    mileage_col <- 22
    condition_col <- 17:19
    wheelSize_col <- 154:161
    isOneOwner_col <- 20:21
    color_col <- 46:60
    featureCount_col <- 162
    region_col <- 132:141
    soundSystem_col <- 142:148
    wheelType_col <- 149:153
    
    interact_terms <- cbind(
        displacement_year <- matrix_product(lasso_matrix, displacement_col, year_col),
        year_trim <- matrix_product(lasso_matrix, year_col, trim_col),
        displacement_mileage <- matrix_product(lasso_matrix, displacement_col, mileage_col),
        trim_mileage <- matrix_product(lasso_matrix, trim_col, mileage_col),
        displacement_condition <- matrix_product(lasso_matrix, displacement_col, condition_col),
        year_condition <- matrix_product(lasso_matrix, year_col, condition_col),
        year_mileage <- matrix_product(lasso_matrix, year_col, mileage_col),
        wheelSize_year <- matrix_product(lasso_matrix, wheelSize_col, year_col),
        year_isOneOwner <- matrix_product(lasso_matrix, year_col, isOneOwner_col),
        color_year <- matrix_product(lasso_matrix, color_col, year_col),
        featureCount_year <- matrix_product(lasso_matrix, featureCount_col, year_col),
        region_year <- matrix_product(lasso_matrix, region_col, year_col),
        soundSystem_year <- matrix_product(lasso_matrix, soundSystem_col, year_col),
        wheelType_year <- matrix_product(lasso_matrix, wheelType_col, year_col),
        trim_condition <- matrix_product(lasso_matrix, trim_col, condition_col),
        condition_mileage <- matrix_product(lasso_matrix, condition_col, mileage_col),
        isOneOwner_mileage <- matrix_product(lasso_matrix, isOneOwner_col, mileage_col),
        soundSystem_mileage <- matrix_product(lasso_matrix, soundSystem_col, mileage_col),
        wheelSize_mileage <- matrix_product(lasso_matrix, wheelSize_col, mileage_col),
        color_mileage <- matrix_product(lasso_matrix, color_col, mileage_col),
        featureCount_mileage <- matrix_product(lasso_matrix, featureCount_col, mileage_col),
        region_mileage <- matrix_product(lasso_matrix, region_col, mileage_col),
        wheelType_mileage <- matrix_product(lasso_matrix, wheelType_col, mileage_col)
    )

    
    RMSE <- mean((predict(lasso_model, cbind(
        lasso_matrix[, -c(1, 163)],
        interact_terms)) - lasso_matrix[, "price"])^2)^.5
    
    # clean up variables to free memory
    rm(interact_terms)
    # rm(RMSE)
    gc()
    
    return(RMSE)
}
```
The last function `lasso_prediction` is designed to predict `price` on the `Cars_X_out.csv` data. The the predicted $\hat{price}$ will be populate the `price` column of `raw_data`.
```{r, message=FALSE, tidy=TRUE}
lasso_prediction <- function(lasso_model, lasso_set, raw_data){
    lasso_matrix <- matrix(as.numeric(lasso_set), ncol=ncol(lasso_set))
    
    colnames(lasso_matrix) <- colnames(lasso_set)
    
    displacement_col <- 61:75
    year_col <- 23:45
    trim_col <- 2:14
    mileage_col <- 22
    condition_col <- 17:19
    wheelSize_col <- 154:161
    isOneOwner_col <- 20:21
    color_col <- 46:60
    featureCount_col <- 162
    region_col <- 132:141
    soundSystem_col <- 142:148
    wheelType_col <- 149:153
    
    interact_terms <- cbind(
        displacement_year <- matrix_product(lasso_matrix, displacement_col, year_col),
        year_trim <- matrix_product(lasso_matrix, year_col, trim_col),
        displacement_mileage <- matrix_product(lasso_matrix, displacement_col, mileage_col),
        trim_mileage <- matrix_product(lasso_matrix, trim_col, mileage_col),
        displacement_condition <- matrix_product(lasso_matrix, displacement_col, condition_col),
        year_condition <- matrix_product(lasso_matrix, year_col, condition_col),
        year_mileage <- matrix_product(lasso_matrix, year_col, mileage_col),
        wheelSize_year <- matrix_product(lasso_matrix, wheelSize_col, year_col),
        year_isOneOwner <- matrix_product(lasso_matrix, year_col, isOneOwner_col),
        color_year <- matrix_product(lasso_matrix, color_col, year_col),
        featureCount_year <- matrix_product(lasso_matrix, featureCount_col, year_col),
        region_year <- matrix_product(lasso_matrix, region_col, year_col),
        soundSystem_year <- matrix_product(lasso_matrix, soundSystem_col, year_col),
        wheelType_year <- matrix_product(lasso_matrix, wheelType_col, year_col),
        trim_condition <- matrix_product(lasso_matrix, trim_col, condition_col),
        condition_mileage <- matrix_product(lasso_matrix, condition_col, mileage_col),
        isOneOwner_mileage <- matrix_product(lasso_matrix, isOneOwner_col, mileage_col),
        soundSystem_mileage <- matrix_product(lasso_matrix, soundSystem_col, mileage_col),
        wheelSize_mileage <- matrix_product(lasso_matrix, wheelSize_col, mileage_col),
        color_mileage <- matrix_product(lasso_matrix, color_col, mileage_col),
        featureCount_mileage <- matrix_product(lasso_matrix, featureCount_col, mileage_col),
        region_mileage <- matrix_product(lasso_matrix, region_col, mileage_col),
        wheelType_mileage <- matrix_product(lasso_matrix, wheelType_col, mileage_col)
    )

    
    raw_data[, "price"] <- predict(lasso_model, cbind(lasso_matrix[, -c(1, 163)],interact_terms))
    
    rm(interact_terms)
    gc()
    
    return(raw_data)
}
```

## 2.2. Generate Lasso regression model based on training data

The training data and test data used in our study are separated randomly in a ratio of 80/20, and stored in 2 separate files, which are accessed with `read.csv` function.
To obtain the regression model, the training data is read  from `csv` file.
```{r, highlight=TRUE, warning=FALSE, tidy=TRUE}
#define the column variable types
col_classes <- c("factor",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "numeric",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "factor",
                 "numeric",
                 "numeric")

input_data <- read.csv("group_train.csv",
                       header=T,
                       sep=",",
                       na.strings="unsp",
                       colClasses=col_classes)

input_data$mileage <- log(as.numeric(input_data$mileage))
input_data$featureCount <- (as.numeric(input_data$featureCount))^.5

test_set <- read.csv("group_test.csv",
                     header=T,
                     sep=",",
                     na.strings="unsp",
                     colClasses=col_classes)

test_set$mileage <- log(as.numeric(test_set$mileage))
test_set$featureCount <- (as.numeric(test_set$featureCount))^.5
```
As indicated by the output above, the _in sample RMSE_ from Lasso regression is roughly in the range of 7500~7600, and the variability is largely due to the random nature of how `cv.glmnet` does k-fold cross validation.

With necessary functions all defined above, to build the Lasso regression model, simply pass the raw data to `data_pruning` function and use the returned matrix in `lasso_fit` function. What we have then is the fitted model.
```{r, highlight=TRUE, warning=FALSE, tidy=TRUE}
training_set_4lasso <- data_pruning(input_data)
training_set_4boosting <- as.data.frame(training_set_4lasso)
lasso_model <- lasso_fit(training_set_4lasso)
```

```{r, tidy=TRUE}
rm(boost.model)
gc()

boost.model_1 <- gbm(price ~ trim
                     +condition
                     +isOneOwner
                     +color
                     +displacement
                     +fuel
                     +state
                     +region
                     +soundSystem
                     +wheelType
                     +wheelSize
                     +featureCount
                     +year*displacement
                      +trim*year
                      +mileage*displacement
                      +mileage*trim
                      +condition*displacement
                      +condition*year
                      +mileage*year
                      +year*wheelSize
                      +isOneOwner*year
                      +year*color
                      +year*featureCount
                      +year*region
                      +year*soundSystem
                      +year*wheelType
                      +year
                      +condition*trim
                      +mileage*condition
                      +mileage*isOneOwner
                      +mileage*soundSystem
                      +mileage*wheelSize
                      +mileage*color
                      +mileage*featureCount
                      +mileage*region
                      +mileage*wheelType
                      +mileage,
    data = input_data[, -c(1, 3)],
    distribution="gaussian",
    n.trees = 1000,
    interaction.depth = 5,
    n.minobsinnode = 10,
    cv.folds=10,
    shrinkage = 0.01,
    n.cores=4,
    verbose=FALSE
    )

boost.predict_1 = predict(boost.model_1,
         newdata = input_data,
         n.trees = 1000,
         type="link",
         single.tree=FALSE)

train.RMSE_1 <- round(sqrt(mean((boost.predict_1 - input_data$price)^2)))
train.RMSE_1

boost.predict_1 = predict(boost.model_1,
         newdata = test_set,
         n.trees = 1000,
         type="link",
         single.tree=FALSE)

test.RMSE_1 <- round(sqrt(mean((boost.predict_1 - test_set$price)^2)))
test.RMSE_1

# rm(boost.model)
# gc()
# 
# boost.model_2 <- gbm(price ~.,
#                         data = input_data[, -c(1, 3)],
#                         distribution="gaussian",
#                         n.trees = 1000,
#                         interaction.depth = 5,
#                         n.minobsinnode = 10,
#                         cv.folds=10,
#                         shrinkage = 0.01,
#                         n.cores=4,
#                         verbose=FALSE
#                         )
# 
# boost.predict_2 = predict(boost.model_2,
#          newdata = input_data,
#          n.trees = 1000,
#          type="link",
#          single.tree=FALSE)
# 
# train.RMSE_2 <- round(sqrt(mean((boost.predict_2 - input_data$price)^2)))
# train.RMSE_2
# 
# boost.predict_2 = predict(boost.model_2,
#          newdata = test_set,
#          n.trees = 1000,
#          type="link",
#          single.tree=FALSE)
# 
# test.RMSE_2 <- round(sqrt(mean((boost.predict_2 - test_set$price)^2)))
# test.RMSE_2

```

The _out of sample RMSE_, on the other hand, is usually larger, in the range of 8000~8100. This is understandable given the new information and combinations of predictors in the test data.

## 2.3. Predict $price$ with test data

To make prediction on the test data, pass the test data, training data and column names of the transformed training data matrix to `test_data_pruning`. The returned matrix is taken by `lasso_prediction_RMSE` function with the generated regression model, and `RMSE` will be printed to screen.
```{r, highlight=TRUE, warning=FALSE, tidy=TRUE}
test_set_4lasso <- test_data_pruning(test_set, input_data, colnames(training_set_4lasso))
RMSE_out <- lasso_prediction_RMSE(lasso_model, test_set_4lasso)
rm(test_set)
rm(test_set_4lasso)
gc()
cat("Out of sample RMSE:", RMSE_out)
```

## 2.4. Predict $price$ with `Cars_X_out.csv` data.
The `Cars_X_out.csv` data contain all the predictor variables, but not `price`. Consequently, there is a missing column that will cause dimension mismatch using the above functions, and therefore we assign a zero column at the end of the data frame with the name `price`.
The prepared data frame is pruned and passed to `lasso_prediction` function, and a new data frame is returned containing predicted prices, which is saved in a `csv` file.
```{r, highlight=TRUE, warning=FALSE, tidy=TRUE}
col_classes <- c("factor",
                 "character",
                 "character",
                 "character",
                 "character",
                 "numeric",
                 "character",
                 "character",
                 "character",
                 "character",
                 "character",
                 "character",
                 "character",
                 "character",
                 "character",
                 "numeric"
                 )
out_data <- read.csv("Cars_X_out.csv",
                       header=T,
                       sep=",",
                       na.strings="unsp",
                       colClasses=col_classes)

out_data <- cbind(out_data, as.numeric(0))
colnames(out_data)[ncol(out_data)] <- "price"
prediction_set_4lasso <- test_data_pruning(out_data, input_data, colnames(training_set_4lasso))

rm(input_data)
rm(training_set_4lasso)
gc()

cars_out_price <- lasso_prediction(lasso_model, prediction_set_4lasso, out_data)

write.csv(cars_out_price, "Cars_out_price.csv")
```
To compute the `RMSE` on this data set, please open the `csv` file, and utilize other tools such as _Excel_ and _LibreOffice Calc_---simply because it is easier and more straight forward.

## 2.5. Improving prediction accuracy
We have conducted experiments to improve the prediction accuracy in terms of _in sample RMSE_ and _out of sample RMSE_. Yet constrained by our computers' power and, more honestly, the researchers' diminishing patience, we will not display all the trial results, but a succinct summary.

* K-fold cross validation
    The `cv.glmnet` function has a built-in feature of doing k-fold cross validation, and the number of cuts can be specified in the `nfolds` parameter. Reflecting upon the effect of having a finer cut on the sample, we have tried to increase `nfolds` from the default 10 to 50, but the improvement is merely visible, and therefore we have settled down upon the default 10.
* More interaction terms
    Since Lasso regression penalises all predictors, it is less harmful to give it more interaction terms to play with. But but but, when we unfolded the categorical variables into logical dummy variables, the dimension of the data increased from 17 columns to 160+ columns, and thus adding more columns into the data foreshadows a potential computation disaster. For example, we have tried create interaction terms between `mileage`, `trim`, `year`, `condition` (and a couple of more variables ) and all remaining variables. The interaction terms themselves will grow so large that a ordinary laptop will be deprived of RAM and CPU performance during the modeling process. So yes, we agree that simpler is better at the moment.
* Outliers
    Outliers are among the usual suspects causing wild predictions. But to be short, they are not removed in this case. Yet it is totally feasible though. What is needed is compare the residuals from the `cv.glmnet` output, and record the indices of large variances, and tease out these records in the input data set, and then re-run the modeling process. However, we are not only lazy to do so, but interested in how much information outliers could provide.
    In our case, assuming the records are all free from typo, which should be, then the prices do not only reflect the influences of the given predictors, but information beyond that. For example, some cars might be sold at a higher price due to the pitching skill of some agent, and vice versa. Such information is not directly reflected in the data set, but nonetheless could be captured, probably by creating more interaction terms. Which is also likely the reason why a perfect regression model is a rare cure. Therefore outliers and _human factors_ are respected.
* Missing values
    Missing values are uncertainty and more often unhelpful. There are theories to handle these naughty creatures, but again, they are left intact in our case. But instead we created a separate column for each of them, as a compromised solution. Given more time, we might work out a better way. But looking at the actual data set, many missing values are in those not-so-important columns, such as `subtrim`, and we are comfortable with the proportion of missing values in the whole data set.

# 3. Conclusion
In this study, we have investigated the `Cars.csv` data set, and applied transformation on the raw data for regression purposes. With Lasso method, a regression model is generated by cross validation, and the _in sample RMSE_ is proved to be around 7600. The predicted `price` using this model on test data has a larger mean deviation, with _out of sample RMSE_ around 8100.
To further improve the prediction accuracy, it is possible to generate more interaction terms at the cost of computational burden. But we believe a more reliable way is accessing other predictors such as which store was each car sold and by which agent.

# 4. Acknowledgement
Many many thanks for Carlos' lively instructions, and everyone who offered advice on our project.
Also, thank you Watson.

# 5. Reference

* An Introduction to Statistical Learning with Applications in R 4th Edition, _Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani_, 2014
* Printouts by _Carlos Cavalho_
* www.ibm.com/watson