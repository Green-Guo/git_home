{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import re, math, functools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from patsy import dmatrices\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as stpwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rd.seed(6)\n",
    "lmtz = WordNetLemmatizer().lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print \"{:>10}:{:>10.3f} seconds\".format(func.__name__, t2-t1)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def readData(portion):\n",
    "    skip = rd.sample(xrange(1, 19999), int(math.ceil(19999*(1-portion))))\n",
    "    data = pd.read_csv(\"yelp.csv\", skiprows=skip)\n",
    "    data[\"target\"]=data.stars.map(lambda v: 1 if v>3 else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=readData(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def generateTrainTest(data, portion):\n",
    "    train_index = rd.sample(xrange(len(data)), int(math.ceil(len(data)*portion)))\n",
    "    test_index = list(set(xrange(len(data)))-set(train_index))\n",
    "    train_data = data.ix[train_index]\n",
    "    test_data = data.ix[test_index]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def generateFormula(data, num=False):\n",
    "    formula = \"target~0\"\n",
    "    for var in data.columns.values.tolist():\n",
    "        if num:\n",
    "            if data[var].dtype == \"int64\" and var not in [\"stars\", \"target\"]:\n",
    "                formula += \"+\"+var\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if var not in [\"stars\", \"target\", \"wc\", \"Review\", \"prediction\"]:\n",
    "                formula += \"+\"+var\n",
    "            else:\n",
    "                continue\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_model(data, num=False):\n",
    "    Y, X = dmatrices(generateFormula(data, num=num), data=data, return_type=\"dataframe\")\n",
    "    model = LogisticRegression(random_state=128)\n",
    "    model.fit(X, np.ravel(Y))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predAccuracy(model, data, num):\n",
    "    y, x = dmatrices(generateFormula(data, num=num), data=data, return_type=\"dataframe\")\n",
    "    print \"Accuracy: {:>6.4f}\".format((model.predict(x) == np.ravel(y)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review2wc(text, lem=False):\n",
    "    wc = {}\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    if lem:\n",
    "        lmtzi = lmtz\n",
    "        tokens = map(lmtz, tokens)\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            try:\n",
    "                wc[token] =+ 1\n",
    "            except KeyError:\n",
    "                wc[token] = 1\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def term_prob(corpus, subset):\n",
    "    prob_dict = {}\n",
    "    N = sum([i for (_, i) in list(corpus.items())])\n",
    "    for key in corpus:\n",
    "        if key not in subset:\n",
    "            prob_dict[key] = 1.0 / N\n",
    "        else:\n",
    "            prob_dict[key] = subset[key] + 1.0 / N\n",
    "    return prob_dict\n",
    "\n",
    "@timer\n",
    "def log_prob(term_prob_high, term_prob_low):\n",
    "    term_log_prob = {}\n",
    "    log = math.log\n",
    "    for key in term_prob_high:\n",
    "        term_log_prob[key] = log(term_prob_high[key]/term_prob_low[key])\n",
    "    return term_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def token_count(wc):\n",
    "    tc = {}\n",
    "    for dic in wc.tolist():\n",
    "        if len(dic) == 0: continue\n",
    "        for token, count in dic.items():\n",
    "            try:\n",
    "                tc[token] += count\n",
    "            except KeyError:\n",
    "                tc[token] = 1\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def totalscore(text, prior, benchmark):\n",
    "    prob = 0\n",
    "    review2wci = review2wc\n",
    "    wc = review2wci(text)\n",
    "    for word, count in wc.items():\n",
    "        try:\n",
    "            prob += count * benchmark[word]\n",
    "        except KeyError:\n",
    "            prob += 0\n",
    "    # add log(prior)\n",
    "    prob += math.log(prior/(1-prior+0.00001))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vecCosine(base, test):\n",
    "    product = 0\n",
    "    len_base = math.sqrt(sum(map(lambda x: x*x, base.values())))\n",
    "    len_test = math.sqrt(sum(map(lambda x: x*x, test.values())))\n",
    "    for key in base.keys():\n",
    "        try:\n",
    "            product += base[key] * test[key]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return product\n",
    "\n",
    "@timer\n",
    "def predStar(train, test):\n",
    "    base_vec = train.wc.tolist()\n",
    "    prediction = [0]*len(test)\n",
    "    test_vec = test.wc.tolist()\n",
    "    for i in range(len(test_vec)):\n",
    "        vecCosine_partial = functools.partial(vecCosine, test=test_vec[i])\n",
    "        cosine_list = map(vecCosine_partial, base_vec)\n",
    "        top_5 = sorted(range(len(base_vec)), key=lambda i: cosine_list[i], reverse=True)[:5]\n",
    "        prediction[i] = train.ix[top_5].stars.mean()\n",
    "    pred_star = np.array(map(lambda x: (x>3)*1, prediction))\n",
    "    return pred_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task A. Ignore the text (reviews) and run a classification model with the numeric data (you can use standard methods like logistic regression, k-nearest neighbors or anything else). What is the best accuracy of your model with numeric data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7056\n"
     ]
    }
   ],
   "source": [
    "data = readData(0.2)\n",
    "train, test = generateTrainTest(data, 0.7)\n",
    "model_1 = logistic_model(train, num=True)\n",
    "predAccuracy(model_1, test, num=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B. Perform a supervised classification on a subset of the corpus using the reviews only. You can write your code in Python or R. What accuracy do you get from this text mining exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"wc\"] = data.Review.map(review2wc)\n",
    "token_count_total = token_count(data.wc)\n",
    "token_count_high = token_count(data[data[\"target\"]==1].wc)\n",
    "token_count_low = token_count(data[data[\"target\"]==0].wc)\n",
    "term_prob_high = term_prob(token_count_total, token_count_high)\n",
    "term_prob_low = term_prob(token_count_total, token_count_low)\n",
    "term_log_prob = log_prob(term_prob_high, term_prob_low)\n",
    "totalscore_partial = functools.partial(totalscore, prior=len(data[data.target==1])*1.0/len(data), benchmark=term_log_prob)\n",
    "data[\"review_score\"] = data.Review.map(totalscore_partial)\n",
    "prediction = data.review_score.map(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8617\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy: {:>6.4f}\".format((prediction == data.target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task C. Combine the numeric data and the text classification model (in task B) to create a “hybrid” model. It is your task to figure out how to do this. Now run this hybrid classification model and compare the results with those in A and B. Does the numeric data add to the predictive power relative to text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "train, test = generateTrainTest(data, 0.7)\n",
    "model_2 = logistic_model(train, num=False)\n",
    "predAccuracy(model_2, test, num=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task D. Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and use the sentiment scores to predict high/low rating. Compare and contrast the results of tasks B and D. What can you conclude from your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. take test set\n",
    "2. measure length, create an empty list to store predictions\n",
    "3. compare vector angle between wc and base line by line \n",
    "4. take the top 5 best matches and take average of stars, if average>3, predict 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predStar(train, test)\n",
    "print \"Accuracy: {:>6.4f}\".format((pred == test.target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task E. Implement the PMI approach to sentiment analysis (in either Python or R), and run the classification model with the sentiment scores. How do your results compare with those in Task D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review2list(text):\n",
    "    wc = {}\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    remove = tokens.remove\n",
    "    while \"\" in tokens:\n",
    "        remove(\"\")\n",
    "    for token in tokens:\n",
    "        if token in stopwords:\n",
    "            remove(token)\n",
    "    token_pos = pos_tag(tokens)\n",
    "    pos_list = map(lambda tup: tup[1])\n",
    "    pos_bigram = bigrams(pos_list)\n",
    "    return wc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
