{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = \"Billy Yuan, Nikita Lakhotia, Stuti Maddan, Tyler Nicolas, Wenduo Wang\"\n",
    "__copyright__ = \"Well, knowledge is open to curious minds.\"\n",
    "__license__ = \"GPL-3.0\"\n",
    "__version__ = \"0.2\"\n",
    "__maintainer__ = \"Wenduo Wang\"\n",
    "__email__ = \"wenduo.wang@utexas.edu\"\n",
    "__status__ = \"development\"\n",
    "__date__ = \"Sep/16/2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import time, re, math, functools, cProfile\n",
    "import requests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from patsy import dmatrices\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as stpwds\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a lemmatizer just in case it will be used\n",
    "lmtz = WordNetLemmatizer().lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    '''This is a decorator to return a function's running time'''\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print \"{:>10}:{:>10.3f} seconds\".format(func.__name__, t2-t1)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def readData(portion, random_state=time.time()):\n",
    "    '''Read in a certain portion of data in a random manner'''\n",
    "    rd.seed(random_state)\n",
    "    skip = rd.sample(xrange(1, 19999), int(math.ceil(19999*(1-portion))))\n",
    "    data = pd.read_csv(\"yelp.csv\", skiprows=skip)\n",
    "    data[\"target\"]=data.stars.map(lambda v: 1 if v>3 else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def generateTrainTest(data, portion, random_state=time.time()):\n",
    "    '''Split train and test data set'''\n",
    "    rd.seed(random_state)\n",
    "    train_index = rd.sample(xrange(len(data)), int(math.ceil(len(data)*portion)))\n",
    "    test_index = list(set(xrange(len(data)))-set(train_index))\n",
    "    train_data = data.ix[train_index]\n",
    "    test_data = data.ix[test_index]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def generateFormula(data):\n",
    "    '''A helper function to generate formula for regression'''\n",
    "    formula = \"target~0\"\n",
    "    for var in data.columns.values.tolist():\n",
    "        if data[var].dtype in [\"int64\", \"float64\"] and var not in [\"stars\", \"target\", \"wc\", \"Review\", \"prediction\"]:\n",
    "                \n",
    "            formula += \"+\"+var\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitXY(data):\n",
    "    '''Split independent and dependent variables, and return X as DataFrame Y as Series'''\n",
    "    Y, X = dmatrices(generateFormula(data), data=data, return_type=\"dataframe\")\n",
    "    return X, np.ravel(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_model(X, y):\n",
    "    '''A wrapper to generate and fit a logistic regression model'''\n",
    "    model = LogisticRegression(random_state=128)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printAccuracy(prediction, target):\n",
    "    '''Calculate and format accuracy of prediction against target'''\n",
    "    print \"Accuracy: {:>6.4f}\".format((prediction == target).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review2wc(text, lem=False):\n",
    "    '''Decompose a review into tokens, removing stopwords and optionally do lemmatization'''\n",
    "    wc = {}\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    if lem:\n",
    "        lmtzi = lmtz\n",
    "        tokens = map(lmtz, tokens)\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            try:\n",
    "                wc[token] =+ 1\n",
    "            except KeyError:\n",
    "                wc[token] = 1\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def term_prob(corpus, subset):\n",
    "    '''Given a corpus and a subset, calculate the probability of each word\n",
    "    from the corpus appearing in the subset'''\n",
    "    prob_dict = {}\n",
    "    N = sum([i for (_, i) in list(corpus.items())])\n",
    "    for key in corpus:\n",
    "        if key not in subset:\n",
    "            prob_dict[key] = 1.0 / N\n",
    "        else:\n",
    "            prob_dict[key] = subset[key] + 1.0 / N\n",
    "    return prob_dict\n",
    "\n",
    "@timer\n",
    "def log_prob(term_prob_high, term_prob_low):\n",
    "    '''Given 2 subsets, calculate log relative probability o\n",
    "    a word appearing in subset 1 against in subset 2'''\n",
    "    term_log_prob = {}\n",
    "    log = math.log\n",
    "    for key in term_prob_high:\n",
    "        term_log_prob[key] = log(term_prob_high[key]/term_prob_low[key])\n",
    "    return term_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def token_count(wc):\n",
    "    '''Given a list of dictionaries in the form of \"word:count\",\n",
    "    aggregate word:count in to a single dictionary'''\n",
    "    tc = {}\n",
    "    for dic in wc.tolist():\n",
    "        if len(dic) == 0: continue\n",
    "        for token, count in dic.items():\n",
    "            try:\n",
    "                tc[token] += count\n",
    "            except KeyError:\n",
    "                tc[token] = 1\n",
    "                \n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def totalscore(wc, prior, benchmark):\n",
    "    '''Given a dictionary in the form of \"word:count\", \n",
    "    and reference dictionary in the form of \"word:log relative probability\",\n",
    "    calculate the sum of count*log relative probability,\n",
    "    and at the end add a prior.'''\n",
    "    prob = 0\n",
    "    for word, count in wc.items():\n",
    "        try:\n",
    "            prob += count * benchmark[word]\n",
    "        except KeyError:\n",
    "            prob += 0\n",
    "    prob += math.log(prior/(1-prior+0.00001))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NBClassifier(object):\n",
    "    '''A Naive Bayes classifier object with methods to fit on training data and \n",
    "    predict on test data'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.term_log_prob = None\n",
    "        self.prior = None\n",
    "    \n",
    "    def fit(self, data, x_label, y_label):\n",
    "        '''The core of this method is to keep a dictionary of \"word:log relative probability\"'''\n",
    "        self.X = data[x_label]\n",
    "        self.y = data[y_label]\n",
    "        self.x_label = x_label\n",
    "        self.y_label = y_label\n",
    "        token_count_total = token_count(data[x_label])\n",
    "        token_count_high = token_count(data[data[y_label]==1][x_label])\n",
    "        token_count_low = token_count(data[data[y_label]==0][x_label])\n",
    "        term_prob_high = term_prob(token_count_total, token_count_high)\n",
    "        term_prob_low = term_prob(token_count_total, token_count_low)\n",
    "        self.term_log_prob = log_prob(term_prob_high, term_prob_low)\n",
    "        self.prior = len(data[data[y_label]==1])*1.0/len(data)\n",
    "        \n",
    "    def predict(self, test, threshold=None):\n",
    "        '''Prediction can be tuned by adjusting threshold.\n",
    "        If threshold is set to None, then return actual score.'''\n",
    "        totalscore_partial = functools.partial(totalscore, \n",
    "                                               prior= self.prior,\n",
    "                                               benchmark=self.term_log_prob)\n",
    "        score = test[self.x_label].map(totalscore_partial)\n",
    "        if threshold == None:\n",
    "            return score\n",
    "        else:\n",
    "            prediction = score.map(lambda x: 1 if x>threshold else 0)\n",
    "            return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def positiveness(test, positive, negative, threshold=1):\n",
    "    '''Given a positive vector and a negative vector, \n",
    "    calculate the cosine value between the two and a test vector,\n",
    "    and return the ratio of positive/negative.\n",
    "    Prediction is tuned by threshold.'''\n",
    "    product_positive = 0.1\n",
    "    product_negative = 0.1\n",
    "    len_positive = math.sqrt(sum(map(lambda x: x*x, positive.values())))\n",
    "    len_negative = math.sqrt(sum(map(lambda x: x*x, negative.values())))\n",
    "    for key in positive.keys():\n",
    "        try:\n",
    "            product_positive += positive[key] * test[key]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    product_positive = product_positive*1.0/len_positive    \n",
    "        \n",
    "    for key in negative.keys():\n",
    "        try:\n",
    "            product_negative += negative[key] * test[key]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    product_negative = product_negative*1.0/len_negative\n",
    "    \n",
    "    return ((product_positive*1.0/product_negative)>threshold)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review2pairs(text, pattern_1, pattern_2, no_match):\n",
    "    '''Decompose a review to (token,pos_tag) pairs.\n",
    "    Filter the pairs based on adjacent pos tag specified in pattern_1, pattern_2 and no_match.'''\n",
    "    wc = []\n",
    "    append = wc.append\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    remove = tokens.remove\n",
    "    while \"\" in tokens:\n",
    "        remove(\"\")\n",
    "    for token in tokens:\n",
    "        if token in stopwords:\n",
    "            remove(token)\n",
    "    token_pos = pos_tag(tokens)\n",
    "    \n",
    "    for i in xrange(len(token_pos)-1):\n",
    "        if (token_pos[i][1], token_pos[i+1][1]) not in pattern_1 and\\\n",
    "            (token_pos[i][1], token_pos[i+1][1]) not in pattern_2:\n",
    "                continue\n",
    "        elif (token_pos[i][1], token_pos[i+1][1]) in pattern_1:\n",
    "            append((token_pos[i][0], token_pos[i+1][0]))\n",
    "        elif (token_pos[i][1], token_pos[i+1][1]) in pattern_2:\n",
    "            try:\n",
    "                if token_pos[i+2] not in no_match:\n",
    "                    append((token_pos[i][0], token_pos[i+1][0]))\n",
    "            except IndexError:\n",
    "                append((token_pos[i][0], token_pos[i+1][0]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semanticOrientation(phrases,\n",
    "                        positive=\"excellent\", \n",
    "                        negative=\"poor\", \n",
    "                        prior = 1,\n",
    "                        engine=\"google\",\n",
    "                        distance=5,\n",
    "                        threshold=None):\n",
    "    '''Warning: this is a scraper function, please be aware of the website scraping policy.\n",
    "    Google is recommended over bing.\n",
    "    Given a list of phrases in the form of (word1, word2), find the average semantic orientation\n",
    "    of hits(phrase AROUND(distance) positive)/hits(phrase AROUND(distance) negative)/prior\n",
    "    Hits is the number of search result from the engine.\n",
    "    By default prior is set to 1, which should strictly be the ratio of hits(positive)/hits(negative).\n",
    "    '''\n",
    "    so_positive = 0.01\n",
    "    so_negative = 0.01\n",
    "    so_avg = 0\n",
    "    get = requests.get\n",
    "\n",
    "    if engine.lower() == \"google\":\n",
    "        url = \"http://www.google.com/search?q=%s\"\n",
    "        id_pattern = \"resultStats\"\n",
    "    elif engine.lower() == \"bing\":\n",
    "        url = \"http://www.bing.com/search?q=%s\"\n",
    "        id_pattern = \"b_tween\"\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for phrase in phrases:\n",
    "        term = \"%22{}+{}%22+AROUND({})+%22{}%22\".format(phrase[0], distance, phrase[1], positive)\n",
    "        page = get(url % term)\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        try:\n",
    "            rtr_pos = int(\"\".join(re.split(\"\\D+\",soup.find(\"div\", id=id_pattern).get_text().encode(\"utf-８\"))))\n",
    "        except AttributeError:\n",
    "            rtr_pos = 0\n",
    "            \n",
    "        if rtr_pos == None:\n",
    "            rtr_pos = 0\n",
    "        \n",
    "        term = \"%22{}+{}%22+AROUND({})+%22{}%22\".format(phrase[0], distance, phrase[1], negative)\n",
    "        page = get(url % term)\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        try:\n",
    "            rtr_neg = int(\"\".join(re.split(\"\\D+\",soup.find(\"div\", id=id_pattern).get_text().encode(\"utf-８\"))))\n",
    "        except AttributeError:\n",
    "            rtr_neg = 0\n",
    "            \n",
    "        if rtr_neg == None:\n",
    "            rtr_neg =0\n",
    "            \n",
    "        so_positive += rtr_pos\n",
    "        so_negative += rtr_neg\n",
    "        \n",
    "        so_avg += math.log(so_positive/so_negative/prior)\n",
    "    \n",
    "    so_avg = so_avg*1.0/len(phrases)\n",
    "    \n",
    "    if threshold == None:\n",
    "        return so_avg\n",
    "    else:\n",
    "        return int(so_avg>threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task A. Ignore the text (reviews) and run a classification model with the numeric data (you can use standard methods like logistic regression, k-nearest neighbors or anything else). What is the best accuracy of your model with numeric data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just try logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  readData:     0.099 seconds\n",
      "generateTrainTest:     0.009 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "Accuracy: 0.6706\n"
     ]
    }
   ],
   "source": [
    "data = readData(0.2, random_state=8)\n",
    "train_1, test_1 = generateTrainTest(data, 0.7, random_state=8)\n",
    "X_train, y_train = splitXY(train_1)\n",
    "model_1 = logistic_model(X_train, y_train)\n",
    "X_test, y_test = splitXY(test_1)\n",
    "prediction = model_1.predict(X_test)\n",
    "printAccuracy(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B. Perform a supervised classification on a subset of the corpus using the reviews only. You can write your code in Python or R. What accuracy do you get from this text mining exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generateTrainTest:     0.006 seconds\n"
     ]
    }
   ],
   "source": [
    "data[\"wc\"] = data.Review.map(review2wc)\n",
    "train_2, test_2 = generateTrainTest(data, 0.7, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_count:     0.092 seconds\n",
      "token_count:     0.032 seconds\n",
      "token_count:     0.017 seconds\n",
      " term_prob:     0.009 seconds\n",
      " term_prob:     0.008 seconds\n",
      "  log_prob:     0.007 seconds\n",
      "Accuracy: 0.6822\n"
     ]
    }
   ],
   "source": [
    "classifier = NBClassifier()\n",
    "classifier.fit(train_2, \"wc\", \"target\")\n",
    "prediction = classifier.predict(test_2, threshold=3.5)\n",
    "printAccuracy(prediction, test_2.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of NB is similar to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task C. Combine the numeric data and the text classification model (in task B) to create a “hybrid” model. It is your task to figure out how to do this. Now run this hybrid classification model and compare the results with those in A and B. Does the numeric data add to the predictive power relative to text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add a column of scores generated from the Naive Bayes classifier to the data, and rerun logistic regression on all numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generateTrainTest:     0.005 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "Accuracy: 0.6822\n"
     ]
    }
   ],
   "source": [
    "data[\"total_score\"] = classifier.predict(data, threshold=3.5)\n",
    "train_3, test_3 = generateTrainTest(data, 0.7, random_state=8)\n",
    "X_train, y_train = splitXY(train_3)\n",
    "model_2 = logistic_model(X_train, y_train)\n",
    "X_test, y_test = splitXY(test_3)\n",
    "prediction = model_2.predict(X_test)\n",
    "printAccuracy(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is __not__ significantly higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task D. Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and use the sentiment scores to predict high/low rating. Compare and contrast the results of tasks B and D. What can you conclude from your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector angle approach\n",
    "Compare each review with a totally biased positive/negative review,\n",
    "and compare the ratio of $\\frac{\\cos \\langle review, positive \\rangle}{\\cos \n",
    "\\langle review, positive \\rangle}$ with a user specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totally_positive = \"This restaurant is very good. It is actually the best on that I have ever been to.\\\n",
    "                    The queue could be long, but if you have booked well in advance it would not be a problem.\\\n",
    "                    Everyone smiles and their service is definitely professional. The foods are fantastic,\\\n",
    "                    and the price is low, I mean affordable. The wines are very nice, and there is a good collection\\\n",
    "                    of desserts which tastes phenomenal. The waiter and waitress are attentative and helpful.\\\n",
    "                    I believe they have been trained very well. Tables are clean, dishes\\\n",
    "                    served in time and they taste absolutely delicious. I totally recommend it.\"\n",
    "\n",
    "totally_negative = \"I can't believe this restaurant could be so bad. We waited for a long time before we were attended\\\n",
    "                    to by a waiter, who was so crude, maybe because he thought I couldn't afford the meal, the price of\\\n",
    "                    which by the way is riculously high. We each ordered 3 courses, but nothing showed up in the following\\\n",
    "                    30 minutes. Nobody even explained that to us. Finally I called the manager, and he just said they were\\\n",
    "                    busy. Well, I could see they were busy, but it doesn't make sense that other people were served better\\\n",
    "                    than us. And the end, we decided to give a smaller tip to the waitor (I preferred not at all), and\\\n",
    "                    I can still remember his face -- disgusting. Please don't go there!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_vec = review2wc(totally_positive)\n",
    "negative_vec = review2wc(totally_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6564\n"
     ]
    }
   ],
   "source": [
    "positiveness_partial = functools.partial(positiveness, positive=positive_vec, negative=negative_vec, threshold=.5)\n",
    "unsupervised_prediction = data.wc.map(positiveness_partial)\n",
    "printAccuracy(unsupervised_prediction, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task E. Implement the PMI approach to sentiment analysis (in either Python or R), and run the classification model with the sentiment scores. How do your results compare with those in Task D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find word pairs and calculate average semantic orientation based on online search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern_1 = [(\"JJ\", \"NN\"), (\"JJ\", \"NNS\"), \n",
    "           (\"RB\", \"VB\"), (\"RB\", \"VBD\"), (\"RB\", \"VBN\"), (\"RB\", \"VBG\"),\n",
    "          (\"RBR\", \"VB\"), (\"RBR\", \"VBD\"), (\"RBR\", \"VBN\"), (\"RBR\", \"VBG\"),\n",
    "          (\"RBS\", \"VB\"), (\"RBS\", \"VBD\"), (\"RBS\", \"VBN\"), (\"RBS\", \"VBG\")]\n",
    "pattern_2 = [(\"RB\", \"JJ\"), (\"RBR\", \"JJ\"), (\"RBS\", \"JJ\"),\n",
    "            (\"JJ\", \"JJ\"),\n",
    "            (\"NN\", \"JJ\"), (\"NNS\", \"JJ\")]\n",
    "no_match = [\"NN\", \"NNS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the review and keep word pairs whose pos tags match the above specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r2p = functools.partial(review2pairs, pattern_1=pattern_1, pattern_2=pattern_2, no_match=no_match)\n",
    "pairs=data.Review.map(r2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the filtered word pairs into search engine, and find if they are more related with positive or negative sentiments.\n",
    "\n",
    "> Google will block IP if search requests exceed 40/hr. So be careful not to run this too often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so = functools.partial(semanticOrientation,\n",
    "                        positive=\"excellent\", \n",
    "                        negative=\"poor\", \n",
    "                        prior = 1,\n",
    "                        engine=\"google\",\n",
    "                        distance=5,\n",
    "                        threshold=None)\n",
    "prediction = pairs[:10].map(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "printAccuracy(prediction, data.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task F. What are the top 5 “attributes” of a restaurant that are associated with (i) high and (ii) low ratings? That is, when people rate a restaurant high or low, are they more likely to mention service, ambiance, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector angle approach\n",
    "Define several topics with common words, and compare the review with each topic. Calculate the cosine value as a proximity indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food = \"delicious, food, salad, dessert, tasty, melt, wine, disgusting, dish, flavor, crunchy, yummy, appetizing,\\\n",
    "        course, cheese, meat, beef, steak, lamb, soup, cooked, cook, charred, burn, burned, burnt, burning, hot,\\\n",
    "        spicy, fresh, seasoning, seasoned, marinated, crisp, tender, crust\"\n",
    "food = review2wc(food)\n",
    "service = \"waitor, waitress, serve, served, service, serving, polite, considerate, careful, attentative, warm,\\\n",
    "            attention, rude, waiting, wait, long, smile, smiling, smiled, patient, inpatient, angry, anger, annoy,\\\n",
    "            annoying, responsive, response\"\n",
    "service = review2wc(service)\n",
    "ambiance = \"ambiance, decoration, lighting, light, lights, music, piano, candle, candles, environment, decorated,\\\n",
    "            table, cloth, glass, violin, performance, comfort, comfortable, soft, sofa, chair, relax, relaxing,\\\n",
    "            romance, romantic, classic, noisy, boyfriend, girlfriend\"\n",
    "ambiance = review2wc(ambiance)\n",
    "price = \"price, affordable, expensive, cheap, worth, worthwhile, worthy, dear, charge, charged, fee, tip, tips,\\\n",
    "            ripped\"\n",
    "price = review2wc(price)\n",
    "convenience = \"parking, drive, convenience, convenient, commute, highway, road, street, crowded, queue, line,\\\n",
    "                traffic, every, week, month, days, day, everyday\"\n",
    "convenience = review2wc(convenience)\n",
    "hygiene = \"hygiene, clean, dirty, safe, tidy, hygienic, cloth, toilet, restroom, washroom, dress, dressed, hair,\\\n",
    "            bug, fly, flies, smelly, stink, rotten, bad, wash, washed, hand, hands\"\n",
    "hygiene = review2wc(hygiene)\n",
    "health = \"healthy, health, calorie, heavy, light, sweet, sugar, fat, oil, salad, salty, salt, energy, refresh,\\\n",
    "            refreshing, heart, body, portion, size, material, materials, ingredient\"\n",
    "health = review2wc(health)\n",
    "family = \"kid, kids, play, playground, child, children, plays, played, baby, seat, son, girl, daughter, dad, mom,\\\n",
    "        mother, father, grandma, grandpa\"\n",
    "family = review2wc(family)\n",
    "party = \"friends, friend, together, party, celebrate, celebration, celebrated, space, room, big, birthday\"\n",
    "party = review2wc(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matchAttribute(wc, attribute):\n",
    "    attr_score = {}\n",
    "    len_wc = math.sqrt(sum(map(lambda x: x*x, wc.values())))\n",
    "    len_attr = math.sqrt(sum(map(lambda x: x*x, attribute.values())))\n",
    "    match = 0\n",
    "    for key in attribute.keys():\n",
    "        try:\n",
    "            match += abs(attribute[key] * wc[key])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    match = match*1.0/len_wc/len_attr\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Populate the dataframe with matching scores.\n",
    "for attr in {\"food\":food, \"service\":service, \"ambiance\":ambiance, \"price\":price, \"convenience\":convenience,\n",
    "            \"hygiene\":hygiene, \"health\":health, \"family\":family, \"party\":party}.items():\n",
    "    \n",
    "    matchAttribute_partial = functools.partial(matchAttribute, attribute=attr[1])\n",
    "    data[\"match_{}\".format(attr[0])] = data.wc.map(matchAttribute_partial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort by higher rated restaurants\n",
      "target                    0         1\n",
      "match_food         0.044216  0.048825\n",
      "match_service      0.022893  0.019416\n",
      "match_party        0.012686  0.013545\n",
      "match_convenience  0.011616  0.013229\n",
      "match_health       0.011030  0.010964\n",
      "match_ambiance     0.008162  0.008941\n",
      "match_price        0.011992  0.008871\n",
      "match_hygiene      0.007786  0.004586\n",
      "match_family       0.004767  0.004296\n",
      "\n",
      "\n",
      "Sort by lower rated restaurants\n",
      "target                    0         1\n",
      "match_food         0.044216  0.048825\n",
      "match_service      0.022893  0.019416\n",
      "match_party        0.012686  0.013545\n",
      "match_price        0.011992  0.008871\n",
      "match_convenience  0.011616  0.013229\n",
      "match_health       0.011030  0.010964\n",
      "match_ambiance     0.008162  0.008941\n",
      "match_hygiene      0.007786  0.004586\n",
      "match_family       0.004767  0.004296\n"
     ]
    }
   ],
   "source": [
    "# Extract matching scores and target.\n",
    "match_cols = [col for col in data.columns.values if \"match\" in col] + [\"target\"]\n",
    "match_df = data[match_cols]\n",
    "match_df = data[match_cols]\n",
    "# Sort topics by matching score.\n",
    "print \"Sort by higher rated restaurants\"\n",
    "print match_df.groupby(\"target\").mean().T.sort_values([1], ascending=False)\n",
    "print \"\\n\"\n",
    "print \"Sort by lower rated restaurants\"\n",
    "print match_df.groupby(\"target\").mean().T.sort_values([0], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
