{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = \"Billy Yuan, Nikita Lakhotia, Stuti Maddan, Tyler Nicolas, Wenduo Wang\"\n",
    "__copyright__ = \"Well, knowledge is open to curious minds.\"\n",
    "__license__ = \"GPL-3.0\"\n",
    "__version__ = \"0.2\"\n",
    "__maintainer__ = \"Wenduo Wang\"\n",
    "__email__ = \"wenduo.wang@utexas.edu\"\n",
    "__status__ = \"development\"\n",
    "__date__ = \"Sep/16/2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import time, re, math, functools, cProfile\n",
    "import requests\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from patsy import dmatrices\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as stpwds\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a lemmatizer just in case it will be used\n",
    "lmtz = WordNetLemmatizer().lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    '''This is a decorator to return a function's running time'''\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print \"{:>10}:{:>10.3f} seconds\".format(func.__name__, t2-t1)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def readData(portion, random_state=time.time()):\n",
    "    '''Read in a certain portion of data in a random manner'''\n",
    "    rd.seed(random_state)\n",
    "    skip = rd.sample(xrange(1, 19999), int(math.ceil(19999*(1-portion))))\n",
    "    data = pd.read_csv(\"yelp.csv\", skiprows=skip)\n",
    "    data[\"target\"]=data.stars.map(lambda v: 1 if v>3 else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def generateTrainTest(data, portion, random_state=time.time()):\n",
    "    '''Split train and test data set'''\n",
    "    rd.seed(random_state)\n",
    "    train_index = rd.sample(xrange(len(data)), int(math.ceil(len(data)*portion)))\n",
    "    test_index = list(set(xrange(len(data)))-set(train_index))\n",
    "    train_data = data.ix[train_index]\n",
    "    test_data = data.ix[test_index]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def generateFormula(data):\n",
    "    '''A helper function to generate formula for regression'''\n",
    "    formula = \"target~0\"\n",
    "    for var in data.columns.values.tolist():\n",
    "        if data[var].dtype == \"int64\" and var not in [\"stars\", \"target\", \"wc\", \"Review\", \"prediction\"]:\n",
    "            formula += \"+\"+var\n",
    "        else:\n",
    "            continue\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitXY(data):\n",
    "    '''Split independent and dependent variables, and return X as DataFrame Y as Series'''\n",
    "    Y, X = dmatrices(generateFormula(data), data=data, return_type=\"dataframe\")\n",
    "    return X, np.ravel(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_model(X, y):\n",
    "    '''A wrapper to generate and fit a logistic regression model'''\n",
    "    model = LogisticRegression(random_state=128)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printAccuracy(prediction, target):\n",
    "    '''Calculate and format accuracy of prediction against target'''\n",
    "    print \"Accuracy: {:>6.4f}\".format((prediction == target).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review2wc(text, lem=False):\n",
    "    '''Decompose a review into tokens, removing stopwords and optionally do lemmatization'''\n",
    "    wc = {}\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    if lem:\n",
    "        lmtzi = lmtz\n",
    "        tokens = map(lmtz, tokens)\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            try:\n",
    "                wc[token] =+ 1\n",
    "            except KeyError:\n",
    "                wc[token] = 1\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def term_prob(corpus, subset):\n",
    "    '''Given a corpus and a subset, calculate the probability of each word\n",
    "    from the corpus appearing in the subset'''\n",
    "    prob_dict = {}\n",
    "    N = sum([i for (_, i) in list(corpus.items())])\n",
    "    for key in corpus:\n",
    "        if key not in subset:\n",
    "            prob_dict[key] = 1.0 / N\n",
    "        else:\n",
    "            prob_dict[key] = subset[key] + 1.0 / N\n",
    "    return prob_dict\n",
    "\n",
    "@timer\n",
    "def log_prob(term_prob_high, term_prob_low):\n",
    "    '''Given 2 subsets, calculate log relative probability o\n",
    "    a word appearing in subset 1 against in subset 2'''\n",
    "    term_log_prob = {}\n",
    "    log = math.log\n",
    "    for key in term_prob_high:\n",
    "        term_log_prob[key] = log(term_prob_high[key]/term_prob_low[key])\n",
    "    return term_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timer\n",
    "def token_count(wc):\n",
    "    '''Given a list of dictionaries in the form of \"word:count\",\n",
    "    aggregate word:count in to a single dictionary'''\n",
    "    tc = {}\n",
    "    for dic in wc.tolist():\n",
    "        if len(dic) == 0: continue\n",
    "        for token, count in dic.items():\n",
    "            try:\n",
    "                tc[token] += count\n",
    "            except KeyError:\n",
    "                tc[token] = 1\n",
    "                \n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def totalscore(wc, prior, benchmark):\n",
    "    '''Given a dictionary in the form of \"word:count\", \n",
    "    and reference dictionary in the form of \"word:log relative probability\",\n",
    "    calculate the sum of count*log relative probability,\n",
    "    and at the end add a prior.'''\n",
    "    prob = 0\n",
    "    for word, count in wc.items():\n",
    "        try:\n",
    "            prob += count * benchmark[word]\n",
    "        except KeyError:\n",
    "            prob += 0\n",
    "    prob += math.log(prior/(1-prior+0.00001))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NBClassifier(object):\n",
    "    '''A Naive Bayes classifier object with methods to fit on training data and \n",
    "    predict on test data'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.term_log_prob = None\n",
    "        self.prior = None\n",
    "    \n",
    "    def fit(self, data, x_label, y_label):\n",
    "        '''The core of this method is to keep a dictionary of \"word:log relative probability\"'''\n",
    "        self.X = data[x_label]\n",
    "        self.y = data[y_label]\n",
    "        self.x_label = x_label\n",
    "        self.y_label = y_label\n",
    "        token_count_total = token_count(data[x_label])\n",
    "        token_count_high = token_count(data[data[y_label]==1][x_label])\n",
    "        token_count_low = token_count(data[data[y_label]==0][x_label])\n",
    "        term_prob_high = term_prob(token_count_total, token_count_high)\n",
    "        term_prob_low = term_prob(token_count_total, token_count_low)\n",
    "        self.term_log_prob = log_prob(term_prob_high, term_prob_low)\n",
    "        self.prior = len(data[data[y_label]==1])*1.0/len(data)\n",
    "        \n",
    "    def predict(self, test, threshold=None):\n",
    "        '''Prediction can be tuned by adjusting threshold.\n",
    "        If threshold is set to None, then return actual score.'''\n",
    "        totalscore_partial = functools.partial(totalscore, \n",
    "                                               prior= self.prior,\n",
    "                                               benchmark=self.term_log_prob)\n",
    "        score = test[self.x_label].map(totalscore_partial)\n",
    "        if threshold == None:\n",
    "            return score\n",
    "        else:\n",
    "            prediction = score.map(lambda x: 1 if x>threshold else 0)\n",
    "            return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def positiveness(test, positive, negative, threshold=1):\n",
    "    '''Given a positive vector and a negative vector, \n",
    "    calculate the cosine value between the two and a test vector,\n",
    "    and return the ratio of positive/negative.\n",
    "    Prediction is tuned by threshold.'''\n",
    "    product_positive = 0.1\n",
    "    product_negative = 0.1\n",
    "    len_positive = math.sqrt(sum(map(lambda x: x*x, positive.values())))\n",
    "    len_negative = math.sqrt(sum(map(lambda x: x*x, negative.values())))\n",
    "    for key in positive.keys():\n",
    "        try:\n",
    "            product_positive += positive[key] * test[key]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    product_positive = product_positive*1.0/len_positive    \n",
    "        \n",
    "    for key in negative.keys():\n",
    "        try:\n",
    "            product_negative += negative[key] * test[key]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    product_negative = product_negative*1.0/len_negative\n",
    "    \n",
    "    return ((product_positive*1.0/product_negative)>threshold)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review2pairs(text, pattern_1=pattern_1, pattern_2=pattern_2, no_match=no_match):\n",
    "    '''Decompose a review to (token,pos_tag) pairs.\n",
    "    Filter the pairs based on adjacent pos tag specified in pattern_1, pattern_2 and no_match.'''\n",
    "    wc = []\n",
    "    append = wc.append\n",
    "    text = text.lower()\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    stopwords = stpwds.words(\"english\")\n",
    "    remove = tokens.remove\n",
    "    while \"\" in tokens:\n",
    "        remove(\"\")\n",
    "    for token in tokens:\n",
    "        if token in stopwords:\n",
    "            remove(token)\n",
    "    token_pos = pos_tag(tokens)\n",
    "    \n",
    "    for i in xrange(len(token_pos)-1):\n",
    "        if (token_pos[i][1], token_pos[i+1][1]) not in pattern_1 and\\\n",
    "            (token_pos[i][1], token_pos[i+1][1]) not in pattern_2:\n",
    "                continue\n",
    "        elif (token_pos[i][1], token_pos[i+1][1]) in pattern_1:\n",
    "            append((token_pos[i][0], token_pos[i+1][0]))\n",
    "        elif (token_pos[i][1], token_pos[i+1][1]) in pattern_2:\n",
    "            try:\n",
    "                if token_pos[i+2] not in no_match:\n",
    "                    append((token_pos[i][0], token_pos[i+1][0]))\n",
    "            except IndexError:\n",
    "                append((token_pos[i][0], token_pos[i+1][0]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semanticOrientation(phrases,\n",
    "                        positive=\"excellent\", \n",
    "                        negative=\"poor\", \n",
    "                        prior = 1,\n",
    "                        engine=\"google\",\n",
    "                        distance=5,\n",
    "                        threshold=0):\n",
    "    '''Warning: this is a scraper function, please be aware of the website scraping policy.\n",
    "    Google is recommended over bing.\n",
    "    Given a list of phrases in the form of (word1, word2), find the average semantic orientation\n",
    "    of hits(phrase AROUND(distance) positive)/hits(phrase AROUND(distance) negative)/prior\n",
    "    Hits is the number of search result from the engine.\n",
    "    By default prior is set to 1, which should strictly be the ratio of hits(positive)/hits(negative).\n",
    "    '''\n",
    "    so_positive = 0.01\n",
    "    so_negative = 0.01\n",
    "    so_avg = 0\n",
    "    get = requests.get\n",
    "\n",
    "    if engine.lower() == \"google\":\n",
    "        url = \"http://www.google.com/search?q=%s\"\n",
    "        id_pattern = \"resultStats\"\n",
    "    elif engine.lower() == \"bing\":\n",
    "        url = \"http://www.bing.com/search?q=%s\"\n",
    "        id_pattern = \"b_tween\"\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for phrase in phrases:\n",
    "        term = \"%22{}+{}%22+AROUND({})+%22{}%22\".format(phrase[0], distance, phrase[1], positive)\n",
    "        page = get(url % term)\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        rtr_pos = int(\"\".join(re.split(\"\\D+\",soup.find(\"div\", id=id_pattern).get_text().encode(\"utf-８\"))))\n",
    "        if rtr_pos == None:\n",
    "            rtr_pos = 0\n",
    "        \n",
    "        term = \"%22{}+{}%22+AROUND({})+%22{}%22\".format(phrase[0], distance, phrase[1], negative)\n",
    "        page = get(url % term)\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        rtr_neg = int(\"\".join(re.split(\"\\D+\",soup.find(\"div\", id=id_pattern).get_text().encode(\"utf-８\"))))\n",
    "        if rtr_neg == None:\n",
    "            rtr_neg =0\n",
    "            \n",
    "        so_positive += rtr_pos\n",
    "        so_negative += rtr_neg\n",
    "        \n",
    "        so_avg += math.log(so_positive/so_negative/prior)\n",
    "    \n",
    "    return int(so_avg>threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task A. Ignore the text (reviews) and run a classification model with the numeric data (you can use standard methods like logistic regression, k-nearest neighbors or anything else). What is the best accuracy of your model with numeric data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  readData:     0.098 seconds\n",
      "generateTrainTest:     0.005 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "Accuracy: 0.6964\n"
     ]
    }
   ],
   "source": [
    "data = readData(0.2, random_state=6)\n",
    "train, test = generateTrainTest(data, 0.7, random_state=6)\n",
    "X, y = splitXY(data)\n",
    "model_1 = logistic_model(X, y)\n",
    "X_test, y_test = splitXY(test)\n",
    "prediction = model_1.predict(X_test)\n",
    "printAccuracy(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B. Perform a supervised classification on a subset of the corpus using the reviews only. You can write your code in Python or R. What accuracy do you get from this text mining exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generateTrainTest:     0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "data[\"wc\"] = data.Review.map(review2wc)\n",
    "train, test = generateTrainTest(data, 0.7, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_count:     0.080 seconds\n",
      "token_count:     0.045 seconds\n",
      "token_count:     0.017 seconds\n",
      " term_prob:     0.008 seconds\n",
      " term_prob:     0.014 seconds\n",
      "  log_prob:     0.009 seconds\n",
      "Accuracy: 0.7048\n"
     ]
    }
   ],
   "source": [
    "classifier = NBClassifier()\n",
    "classifier.fit(train, \"wc\", \"target\")\n",
    "prediction = classifier.predict(test, threshold=0)\n",
    "printAccuracy(prediction, test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task C. Combine the numeric data and the text classification model (in task B) to create a “hybrid” model. It is your task to figure out how to do this. Now run this hybrid classification model and compare the results with those in A and B. Does the numeric data add to the predictive power relative to text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generateTrainTest:     0.005 seconds\n",
      "generateFormula:     0.002 seconds\n",
      "generateFormula:     0.001 seconds\n",
      "Accuracy: 0.6956\n"
     ]
    }
   ],
   "source": [
    "data[\"total_score\"] = classifier.predict(data, threshold=None)\n",
    "train, test = generateTrainTest(data, 0.7, random_state=6)\n",
    "X, y = splitXY(train)\n",
    "model_2 = logistic_model(X, y)\n",
    "X_test, y_test = splitXY(test)\n",
    "prediction = model_2.predict(X_test)\n",
    "printAccuracy(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task D. Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and use the sentiment scores to predict high/low rating. Compare and contrast the results of tasks B and D. What can you conclude from your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totally_positive = \"This restaurant is very good. It is actually the best on that I have ever been to.\\\n",
    "                    The queue could be long, but if you have booked well in advance it would not be a problem.\\\n",
    "                    Everyone smiles and their service is definitely professional. The foods are fantastic,\\\n",
    "                    and the price is low, I mean affordable. The wines are very nice, and there is a good collection\\\n",
    "                    of desserts which tastes phenomenal. The waiter and waitress are attentative and helpful.\\\n",
    "                    I believe they have been trained very well. Tables are clean, dishes\\\n",
    "                    served in time and they taste absolutely delicious. I totally recommend it.\"\n",
    "\n",
    "totally_negative = \"I can't believe this restaurant could be so bad. We waited for a long time before we were attended\\\n",
    "                    to by a waiter, who was so crude, maybe because he thought I couldn't afford the meal, the price of\\\n",
    "                    which by the way is riculously high. We each ordered 3 courses, but nothing showed up in the following\\\n",
    "                    30 minutes. Nobody even explained that to us. Finally I called the manager, and he just said they were\\\n",
    "                    busy. Well, I could see they were busy, but it doesn't make sense that other people were served better\\\n",
    "                    than us. And the end, we decided to give a smaller tip to the waitor (I preferred not at all), and\\\n",
    "                    I can still remember his face -- disgusting. Please don't go there!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_vec = review2wc(totally_positive)\n",
    "negative_vec = review2wc(totally_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6712\n"
     ]
    }
   ],
   "source": [
    "positiveness_partial = functools.partial(positiveness, positive=positive_vec, negative=negative_vec, threshold=.5)\n",
    "unsupervised_prediction = data.wc.map(positiveness_partial)\n",
    "printAccuracy(unsupervised_prediction, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task E. Implement the PMI approach to sentiment analysis (in either Python or R), and run the classification model with the sentiment scores. How do your results compare with those in Task D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern_1 = [(\"JJ\", \"NN\"), (\"JJ\", \"NNS\"), \n",
    "           (\"RB\", \"VB\"), (\"RB\", \"VBD\"), (\"RB\", \"VBN\"), (\"RB\", \"VBG\"),\n",
    "          (\"RBR\", \"VB\"), (\"RBR\", \"VBD\"), (\"RBR\", \"VBN\"), (\"RBR\", \"VBG\"),\n",
    "          (\"RBS\", \"VB\"), (\"RBS\", \"VBD\"), (\"RBS\", \"VBN\"), (\"RBS\", \"VBG\")]\n",
    "pattern_2 = [(\"RB\", \"JJ\"), (\"RBR\", \"JJ\"), (\"RBS\", \"JJ\"),\n",
    "            (\"JJ\", \"JJ\"),\n",
    "            (\"NN\", \"JJ\"), (\"NNS\", \"JJ\")]\n",
    "no_match = [\"NN\", \"NNS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         24678645 function calls (24678644 primitive calls) in 31.583 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     3999    4.525    0.001   31.534    0.008 <ipython-input-112-92efb8c67513>:1(review2pairs)\n",
      "        1    0.000    0.000   31.583   31.583 <string>:1(<module>)\n",
      "     3999    0.016    0.000   25.201    0.006 __init__.py:81(_pos_tag)\n",
      "     3999    0.013    0.000   25.838    0.006 __init__.py:87(pos_tag)\n",
      "     3999    0.016    0.000    0.429    0.000 api.py:201(open)\n",
      "     3999    0.005    0.000    0.008    0.000 api.py:213(encoding)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:1233(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1247(__getitem__)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:1915(get_loc)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:3381(_ensure_index)\n",
      "     3999    0.006    0.000    0.010    0.000 codecs.py:935(getdecoder)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1011(_possibly_cast_to_datetime)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1111(_possibly_infer_to_datetimelike)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1511(_get_dtype_type)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:1575(is_datetime64tz_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1705(is_sparse)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:1710(is_datetimetz)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1717(is_extension_type)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:1731(is_categorical)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:1736(is_categorical_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1745(is_object_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1763(is_list_like)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1788(is_hashable)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1846(_apply_if_callable)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:73(isnull)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:94(_isnull_new)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:986(_possibly_castable)\n",
      "    15996    0.005    0.000    0.005    0.000 compat.py:544(add_py3_data)\n",
      "    11997    0.029    0.000    0.382    0.000 compat.py:559(_decorator)\n",
      "     3999    0.020    0.000    0.146    0.000 data.py:1030(__init__)\n",
      "     3999    0.006    0.000    0.061    0.000 data.py:1087(read)\n",
      "     3999    0.011    0.000    0.021    0.000 data.py:110(split_resource_url)\n",
      "     3999    0.012    0.000    0.055    0.000 data.py:1346(_read)\n",
      "     3999    0.011    0.000    0.157    0.000 data.py:137(normalize_resource_url)\n",
      "     3999    0.007    0.000    0.031    0.000 data.py:1384(_incr_decode)\n",
      "     3999    0.020    0.000    0.090    0.000 data.py:1426(_check_bom)\n",
      "     7998    0.056    0.000    0.253    0.000 data.py:193(normalize_resource_name)\n",
      "     7998    0.020    0.000    0.203    0.000 data.py:290(__init__)\n",
      "     3999    0.013    0.000    0.213    0.000 data.py:311(open)\n",
      "     3999    0.021    0.000    0.193    0.000 data.py:320(join)\n",
      "     3999    0.001    0.000    0.001    0.000 data.py:330(__str__)\n",
      "     3999    0.049    0.000    0.390    0.000 data.py:543(find)\n",
      "     3999    0.016    0.000    0.181    0.000 data.py:719(load)\n",
      "        6    0.000    0.000    0.000    0.000 dtypes.py:74(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:1973(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:1999(_getitem_column)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:2331(_box_item_values)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:2338(_box_col_values)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:1345(_get_item_cache)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:1359(_set_as_cached)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:2641(__finalize__)\n",
      "      3/2    0.000    0.000    0.000    0.000 generic.py:2658(__getattr__)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:2674(__setattr__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:381(_info_axis)\n",
      "       14    0.000    0.000    0.000    0.000 generic.py:7(_check)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:94(__init__)\n",
      "    11997    0.016    0.000    0.090    0.000 genericpath.py:15(exists)\n",
      "     3999    0.010    0.000    0.036    0.000 genericpath.py:26(isfile)\n",
      "     3999    0.005    0.000    0.014    0.000 genericpath.py:38(isdir)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:135(get_values)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:160(mgr_locs)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:1657(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:183(make_block_same_class)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:191(mgr_locs)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:2482(make_block)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:2695(_get_items)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:272(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:301(iget)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3283(get)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3312(iget)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:3778(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:3824(_block)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3884(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3918(asobject)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:77(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:126(is_all_dates)\n",
      "     3999    0.030    0.000    0.624    0.000 perceptron.py:132(__init__)\n",
      "     3999    0.824    0.000   25.185    0.006 perceptron.py:143(tag)\n",
      "     3999    0.010    0.000    0.191    0.000 perceptron.py:203(load)\n",
      "   352646    0.193    0.000    0.275    0.000 perceptron.py:213(normalize)\n",
      "   233878    0.969    0.000    3.518    0.000 perceptron.py:231(_get_features)\n",
      "  3274292    2.124    0.000    2.526    0.000 perceptron.py:236(add)\n",
      "     3999    0.012    0.000    0.012    0.000 perceptron.py:34(__init__)\n",
      "   233878   14.055    0.000   20.417    0.000 perceptron.py:48(predict)\n",
      " 10524510    1.496    0.000    1.496    0.000 perceptron.py:58(<lambda>)\n",
      "    15996    0.099    0.000    0.157    0.000 posixpath.py:336(normpath)\n",
      "    11997    0.015    0.000    0.155    0.000 posixpath.py:365(abspath)\n",
      "    11997    0.006    0.000    0.013    0.000 posixpath.py:59(isabs)\n",
      "    11997    0.036    0.000    0.049    0.000 posixpath.py:68(join)\n",
      "        2    0.000    0.000    0.000    0.000 range.py:435(__len__)\n",
      "     3999    0.005    0.000    0.020    0.000 re.py:134(match)\n",
      "     7998    0.009    0.000    0.050    0.000 re.py:139(search)\n",
      "    11997    0.012    0.000    0.070    0.000 re.py:144(sub)\n",
      "     3999    0.009    0.000    0.189    0.000 re.py:164(split)\n",
      "    27993    0.038    0.000    0.051    0.000 re.py:226(_compile)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:120(__init__)\n",
      "        1    0.000    0.000   31.583   31.583 series.py:2048(map)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:236(from_array)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:247(_constructor)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:270(_set_axis)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:2787(_sanitize_array)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:2804(_try_cast)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:292(_set_subtyp)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:302(name)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:306(name)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:313(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:375(asobject)\n",
      "     3999    0.004    0.000    0.004    0.000 simple.py:100(__init__)\n",
      "     3999    0.113    0.000    0.208    0.000 simple.py:108(tokenize)\n",
      "     3999    0.012    0.000    0.224    0.000 simple.py:132(line_tokenize)\n",
      "     7998    0.002    0.000    0.002    0.000 stat.py:24(S_IFMT)\n",
      "     3999    0.003    0.000    0.003    0.000 stat.py:40(S_ISDIR)\n",
      "     3999    0.005    0.000    0.007    0.000 stat.py:49(S_ISREG)\n",
      "     3999    0.003    0.000    0.004    0.000 urllib.py:1043(_is_unicode)\n",
      "     3999    0.008    0.000    0.012    0.000 urllib.py:1210(unquote)\n",
      "     3999    0.005    0.000    0.017    0.000 urllib.py:53(url2pathname)\n",
      "     3999    0.004    0.000    0.025    0.000 utf_8.py:15(decode)\n",
      "     3999    0.004    0.000    0.005    0.000 util.py:407(concat)\n",
      "     3999    0.009    0.000    0.806    0.000 wordlist.py:19(words)\n",
      "     3999    0.073    0.000    0.573    0.000 wordlist.py:22(raw)\n",
      "     3999    0.004    0.000    0.004    0.000 {_codecs.lookup}\n",
      "     3999    0.021    0.000    0.021    0.000 {_codecs.utf_8_decode}\n",
      "        1    0.000    0.000    0.000    0.000 {callable}\n",
      "       16    0.000    0.000    0.000    0.000 {getattr}\n",
      "        7    0.000    0.000    0.000    0.000 {hasattr}\n",
      "        4    0.000    0.000    0.000    0.000 {hash}\n",
      "    28042    0.014    0.000    0.014    0.000 {isinstance}\n",
      "       10    0.000    0.000    0.000    0.000 {issubclass}\n",
      "   246471    0.025    0.000    0.025    0.000 {len}\n",
      "   233880    2.508    0.000    4.004    0.000 {max}\n",
      "   506312    0.069    0.000    0.069    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "     7998    0.004    0.000    0.004    0.000 {method 'endswith' of 'str' objects}\n",
      "    11997    0.007    0.000    0.007    0.000 {method 'endswith' of 'unicode' objects}\n",
      "   392637    0.124    0.000    0.124    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get_loc' of 'pandas.index.IndexEngine' objects}\n",
      "     3999    0.001    0.000    0.001    0.000 {method 'groups' of '_sre.SRE_Match' objects}\n",
      "   705212    0.044    0.000    0.044    0.000 {method 'isdigit' of 'str' objects}\n",
      "  2953694    2.358    0.000    2.358    0.000 {method 'items' of 'dict' objects}\n",
      "  3278291    0.404    0.000    0.404    0.000 {method 'join' of 'str' objects}\n",
      "    15996    0.013    0.000    0.013    0.000 {method 'join' of 'unicode' objects}\n",
      "   355638    0.047    0.000    0.047    0.000 {method 'lower' of 'str' objects}\n",
      "     3999    0.003    0.000    0.003    0.000 {method 'lstrip' of 'unicode' objects}\n",
      "     3999    0.010    0.000    0.010    0.000 {method 'match' of '_sre.SRE_Pattern' objects}\n",
      "     7998    0.047    0.000    0.047    0.000 {method 'read' of 'file' objects}\n",
      "   191383    0.159    0.000    0.159    0.000 {method 'remove' of 'list' objects}\n",
      "     3999    0.006    0.000    0.006    0.000 {method 'replace' of 'str' objects}\n",
      "    11997    0.009    0.000    0.009    0.000 {method 'replace' of 'unicode' objects}\n",
      "   611847    0.036    0.000    0.036    0.000 {method 'rstrip' of 'unicode' objects}\n",
      "     7998    0.024    0.000    0.024    0.000 {method 'search' of '_sre.SRE_Pattern' objects}\n",
      "     7998    0.028    0.000    0.028    0.000 {method 'seek' of 'file' objects}\n",
      "     3999    0.169    0.000    0.169    0.000 {method 'split' of '_sre.SRE_Pattern' objects}\n",
      "     7998    0.010    0.000    0.010    0.000 {method 'split' of 'str' objects}\n",
      "    15996    0.019    0.000    0.019    0.000 {method 'split' of 'unicode' objects}\n",
      "     3999    0.059    0.000    0.059    0.000 {method 'splitlines' of 'unicode' objects}\n",
      "    27993    0.017    0.000    0.017    0.000 {method 'startswith' of 'str' objects}\n",
      "    47988    0.020    0.000    0.020    0.000 {method 'startswith' of 'unicode' objects}\n",
      "    11997    0.040    0.000    0.040    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {min}\n",
      "        2    0.000    0.000    0.000    0.000 {numpy.core.multiarray.array}\n",
      "     3999    0.043    0.000    0.043    0.000 {open}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas.algos.ensure_object}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas.lib.checknull}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas.lib.infer_dtype}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas.lib.is_possible_datetimelike_array}\n",
      "        2    0.000    0.000    0.000    0.000 {pandas.lib.isscalar}\n",
      "        1    0.048    0.048   31.582   31.582 {pandas.lib.map_infer}\n",
      "        2    0.000    0.000    0.000    0.000 {pandas.lib.values_from_object}\n",
      "    19995    0.100    0.000    0.100    0.000 {posix.stat}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r2p = functools.partial(review2pairs, pattern_1=pattern_1, pattern_2=pattern_2, no_match=no_match)\n",
    "cProfile.run(\"pairs=data.Review.map(r2p)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so = functools.partial(semanticOrientation,\n",
    "                        positive=\"excellent\", \n",
    "                        negative=\"poor\", \n",
    "                        prior = 1,\n",
    "                        engine=\"google\",\n",
    "                        distance=5,\n",
    "                        threshold=0)\n",
    "cProfile.run(\"printAccuracy(pairs[:10].map(so), data.target[:10])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task F. What are the top 5 “attributes” of a restaurant that are associated with (i) high and (ii) low ratings? That is, when people rate a restaurant high or low, are they more likely to mention service, ambiance, etc.? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food = \"delicious, food, salad, dessert, tasty, melt, wine, disgusting, dish, flavor, crunchy, yummy, appetizing,\\\n",
    "        course, cheese, meat, beef, steak, lamb, soup, cooked, cook, charred, burn, burned, burnt, burning, hot,\\\n",
    "        spicy, fresh, seasoning, seasoned, marinated, crisp, tender, crust\"\n",
    "food = review2wc(food)\n",
    "service = \"waitor, waitress, serve, served, service, serving, polite, considerate, careful, attentative, warm,\\\n",
    "            attention, rude, waiting, wait, long, smile, smiling, smiled, patient, inpatient, angry, anger, annoy,\\\n",
    "            annoying, responsive, response\"\n",
    "service = review2wc(service)\n",
    "ambiance = \"ambiance, decoration, lighting, light, lights, music, piano, candle, candles, environment, decorated,\\\n",
    "            table, cloth, glass, violin, performance, comfort, comfortable, soft, sofa, chair, relax, relaxing,\\\n",
    "            romance, romantic, classic, noisy, boyfriend, girlfriend\"\n",
    "ambiance = review2wc(ambiance)\n",
    "price = \"price, affordable, expensive, cheap, worth, worthwhile, worthy, dear, charge, charged, fee, tip, tips,\\\n",
    "            ripped\"\n",
    "price = review2wc(price)\n",
    "convenience = \"parking, drive, convenience, convenient, commute, highway, road, street, crowded, queue, line,\\\n",
    "                traffic, every, week, month, days, day, everyday\"\n",
    "convenience = review2wc(convenience)\n",
    "hygiene = \"hygiene, clean, dirty, safe, tidy, hygienic, cloth, toilet, restroom, washroom, dress, dressed, hair,\\\n",
    "            bug, fly, flies, smelly, stink, rotten, bad, wash, washed, hand, hands\"\n",
    "hygiene = review2wc(hygiene)\n",
    "health = \"healthy, health, calorie, heavy, light, sweet, sugar, fat, oil, salad, salty, salt, energy, refresh,\\\n",
    "            refreshing, heart, body, portion, size, material, materials, ingredient\"\n",
    "health = review2wc(health)\n",
    "family = \"kid, kids, play, playground, child, children, plays, played, baby, seat, son, girl, daughter, dad, mom,\\\n",
    "        mother, father, grandma, grandpa\"\n",
    "family = review2wc(family)\n",
    "party = \"friends, friend, together, party, celebrate, celebration, celebrated, space, room, big, birthday\"\n",
    "party = review2wc(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matchAttribute(wc, attribute):\n",
    "    attr_score = {}\n",
    "    len_wc = math.sqrt(sum(map(lambda x: x*x, wc.values())))\n",
    "    len_attr = math.sqrt(sum(map(lambda x: x*x, attribute.values())))\n",
    "    match = 0\n",
    "    for key in attribute.keys():\n",
    "        try:\n",
    "            match += abs(attribute[key] * wc[key])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    match = match*1.0/len_wc/len_attr\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for attr in {\"food\":food, \"service\":service, \"ambiance\":ambiance, \"price\":price, \"convenience\":convenience,\n",
    "            \"hygiene\":hygiene, \"health\":health, \"family\":family, \"party\":party}.items():\n",
    "    \n",
    "    matchAttribute_partial = functools.partial(matchAttribute, attribute=attr[1])\n",
    "    data[\"match_{}\".format(attr[0])] = data.wc.map(matchAttribute_partial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort by higher rated restaurants\n",
      "target                    0         1\n",
      "match_food         0.043800  0.049359\n",
      "match_service      0.021624  0.019517\n",
      "match_convenience  0.010780  0.012916\n",
      "match_party        0.012663  0.012254\n",
      "match_health       0.010868  0.010185\n",
      "match_price        0.011953  0.008628\n",
      "match_ambiance     0.008559  0.008579\n",
      "match_hygiene      0.008449  0.004751\n",
      "match_family       0.004104  0.003936\n",
      "match_kids         0.003307  0.002730\n",
      "\n",
      "\n",
      "Sort by lower rated restaurants\n",
      "target                    0         1\n",
      "match_food         0.043800  0.049359\n",
      "match_service      0.021624  0.019517\n",
      "match_party        0.012663  0.012254\n",
      "match_price        0.011953  0.008628\n",
      "match_health       0.010868  0.010185\n",
      "match_convenience  0.010780  0.012916\n",
      "match_ambiance     0.008559  0.008579\n",
      "match_hygiene      0.008449  0.004751\n",
      "match_family       0.004104  0.003936\n",
      "match_kids         0.003307  0.002730\n"
     ]
    }
   ],
   "source": [
    "match_cols = [col for col in data.columns.values if \"match\" in col] + [\"target\"]\n",
    "match_df = data[match_cols]\n",
    "match_df = data[match_cols]\n",
    "print \"Sort by higher rated restaurants\"\n",
    "print match_df.groupby(\"target\").mean().T.sort_values([1], ascending=False)\n",
    "print \"\\n\"\n",
    "print \"Sort by lower rated restaurants\"\n",
    "print match_df.groupby(\"target\").mean().T.sort_values([0], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
