{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data from \"gold.txt\" and \"labels.txt\".  \n",
    "Since there are no headers in the files, `names` parameter should be set explicitly.\n",
    "> Duplicate records in both dataframes are kept, for repeated test on the same url provides enables more precise information about the turks' discernibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold = pd.read_table(\"gold.txt\", names=[\"url\", \"category\"]).dropna()\n",
    "labels = pd.read_table(\"labels.txt\", names=[\"turk\", \"url\", \"category\"]).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Split into two DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if a `url` in _labels_ is in _gold_, make a list of unique `url` in gold, and `map` the lambda expression on the `url` series in _labels_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_list = gold[\"url\"].unique()\n",
    "labels_on_gold = labels[labels[\"url\"].map(lambda s: s in url_list)]\n",
    "labels_unknown = labels[labels[\"url\"].map(lambda s: s not in url_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Compute accuracies of turks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since the computation is all on \"gold\" set `url`, \"labels_on_gold\" dataframe is used instead of \"labels\"\n",
    "2. Merge \"labels_on_gold\" with \"gold\" on `url`.\n",
    "3. Create a new column `correct` in the new dataframe, and assign `True` where the \"turk\" rating is the same with the true rating.\n",
    "4. _Optional: drop the rating columns to reduce the size of the dataframe_\n",
    "5. `groupby` on `turk`, and sum up the `True` records on `correct` for each `turk`, the returned value is a series\n",
    "6. `value_counts` on `turk`, a series of total rating numbers is returned.\n",
    "7. Divide the previous two series to get the rating accuracy of each `turk`\n",
    "8. Create a new dataframe \"rater_goodness\" with the total rating number series and rating accuracy series, index by default set as `turk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater_merged = pd.merge(\n",
    "                    labels_on_gold,\n",
    "                    gold,\n",
    "                    left_on=\"url\",\n",
    "                    right_on=\"url\",\n",
    "                    suffixes=[\"_1\", \"_2\"]\n",
    "                )\n",
    "\n",
    "rater_merged[\"correct\"] = rater_merged[\"category_1\"] == rater_merged[\"category_2\"]\n",
    "rater_merged = rater_merged[[\"turk\", \"correct\"]]\n",
    "correct_counts = rater_merged.groupby(\"turk\")[\"correct\"].sum()\n",
    "total_counts = rater_merged[\"turk\"].value_counts()\n",
    "avg_correctness = correct_counts/total_counts\n",
    "rater_goodness = pd.DataFrame({\"number_of_ratings\": total_counts, \"average_correctness\": avg_correctness})\n",
    "rater_goodness[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Odds ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use \"map\" function on `average_correctness` to get $\\frac{average\\ correctness}{1 - average\\ correctness}$\n",
    "2. By definition, when `average_correctness` = 1, the ratio should be assigned `float(\"inf\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater_goodness[\"odds\"] = rater_goodness[\"average_correctness\"].map(lambda x: x/(1.001-x))\n",
    "rater_goodness[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Most accurate turks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use `rater_goodness[\"number of ratings\"]>=20` to select turks who rated at least 20 times.\n",
    "2. Sort the list by `average_correctness` in descending order.\n",
    "3. `.index.values` is optional to return only turks, but for aesthetic reasons it is not applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater_goodness[rater_goodness[\"number_of_ratings\"]>=20].sort_values(by=\"average_correctness\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Rating counts versus accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting `average_correctness` against `number of ratings` makes it easier to have an general idea between the two variables. However, from the plot, it is difficult to identify a clear pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(rater_goodness['number_of_ratings'],\n",
    "     rater_goodness['average_correctness'],\n",
    "     marker='o',\n",
    "     color='blue',\n",
    "     linestyle='None')\n",
    "xlabel('number of ratings')\n",
    "ylabel('average correctness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantitatively measure the linear correlation between number of ratings and average correctness, linear regression is used to draw insights.  \n",
    "From the model summary, it is still difficult to establish reliable linear correlation between the two variables, since the coefficient of number of ratings is not significantly different from zero.\n",
    "> _statsmodels_ and _patsy_ modules are imported for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "y, X = dmatrices('average_correctness ~ number_of_ratings', data=rater_goodness, return_type='dataframe')\n",
    "model = sm.OLS(y, X)\n",
    "result = model.fit()\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Overall predicted odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the cutpoint of top 25% turks in term of number of ratings using `quantile(q=.75)`.\n",
    "2. Make a list of \"turk: number of ratings\"\n",
    "3. Make a mask to select records rated by top 25% turks using `map` function.\n",
    "4. Select from the total \"labels\" data set the records rated by top 25% turks.\n",
    "5. Merge this dataframe with \"labels_unknown\" dataframe on `url` and `category`, duplicates dropped.\n",
    "6. Next merge the resulting dataframe with \"rater_goodness\" dataframe.\n",
    "    + First create a new `turk` column in \"rater_goodness\" dataframe from the index\n",
    "    + Only select the records rated by top 25% turks from \"rater_goodness\" dataframe\n",
    "    + Merge the two dataframe on `turk`\n",
    "    + Drop duplicates and missing values\n",
    "7. `groupby` the resulting dataframe on `url` and `category`.\n",
    "8. Apply `prod()` on `odds` to calculate overall odds by `url` and `category`.\n",
    "> here `odds` is the \"overall odds\" as defined in the assignment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_25_cutpoint = labels_on_gold[\"turk\"].value_counts().quantile(q=.75)\n",
    "turk_list = labels_on_gold[\"turk\"].value_counts()\n",
    "\n",
    "mask_1 = labels_unknown[\"turk\"].map(lambda s: turk_list[s]>=top_25_cutpoint if s in turk_list else False)\n",
    "labels_bytop25 = labels_unknown[mask_1]\n",
    "\n",
    "rater_goodness[\"turk\"] = rater_goodness.index\n",
    "\n",
    "odds_top25 = rater_goodness[rater_goodness[\"turk\"].map(lambda s: turk_list[s]>=top_25_cutpoint if s in turk_list else False)]\n",
    "\n",
    "overall_odds = pd.merge(labels_bytop25,\n",
    "                       odds_top25,\n",
    "                       left_on=\"turk\",\n",
    "                       right_on=\"turk\",\n",
    "                       how=\"left\").dropna()\n",
    "\n",
    "overall_odds.groupby([\"url\", \"category\"])[[\"odds\"]].prod()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Predicted categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a dataframe from the `groupby` object in the last question, containing `url`, `category` and `overall odds`.\n",
    "2. Apply `unstack` to breakdown `category` from index to columns.\n",
    "3. Transpose the dataframe and get `idxmax()` on all columns, i.e. `url`, returned value is a series with `url` as index and _np.array_ (\"odds\", `category`) as values.\n",
    "4. Create a dataframe using the returned series, and convert the _np.array_ into a string column \"top category\" by selecting the second element.\n",
    "5. Create a new \"top odds\" column for the dataframe by `max()` on the transposed dataframe in _step 2_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_odds_df = overall_odds.groupby([\"url\", \"category\"])[[\"odds\"]].prod().unstack(\"category\").T.fillna(0)\n",
    "url_rating = pd.DataFrame(overall_odds_df.idxmax())\n",
    "url_rating[\"top category\"] = url_rating[0].map(lambda s: s[1])\n",
    "url_rating = url_rating.set_index(url_rating.index.values)\n",
    "url_rating[\"top odds\"] = overall_odds_df.max()\n",
    "url_rating = url_rating[[\"top category\", \"top odds\"]]\n",
    "url_rating[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: Predicted categories using more turks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Repeat _Question\\ 7_ and _Question\\ 8_ to create a dataframe where `url` are rated by top 75% turks.\n",
    "    > Here only the \"top category\" column is kept and named `result_75`\n",
    "2. Take out `top category` column from the dataframe from _Question 8_ and rename it `result_25`, and make it a dataframe.\n",
    "3. Merge the two dataframes on index.\n",
    "4. Create a `crosstab` with the two columns as index and columns respectively.\n",
    "5. From the crosstab it can be seen that, the most errors are where the top 25% turks rated \"G\" but the top 75% turks rated \"P\" (836 occurences), \"G\" versus \"R\" (285 occurences), and \"P\" versus \"G\" (229 occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_75_cutpoint = labels_on_gold[\"turk\"].value_counts().quantile(q=.25)\n",
    "\n",
    "mask_2 = labels_unknown[\"turk\"].map(lambda s: turk_list[s]>=top_75_cutpoint if s in turk_list else False)\n",
    "labels_bytop75 = labels_unknown[mask_2]\n",
    "\n",
    "odds_top75 = rater_goodness[rater_goodness[\"turk\"].map(lambda s: turk_list[s]>=top_75_cutpoint if s in turk_list else False)]\n",
    "\n",
    "overall_odds_75 = pd.merge(labels_bytop75,\n",
    "                       odds_top75,\n",
    "                       left_on=\"turk\",\n",
    "                       right_on=\"turk\",\n",
    "                       how=\"left\").dropna()\n",
    "\n",
    "overall_odds_df_75 = overall_odds_75.groupby([\"url\", \"category\"])[[\"odds\"]].prod().unstack(\"category\").T.fillna(0)\n",
    "\n",
    "url_rating_75 = pd.DataFrame(overall_odds_df_75.idxmax())\n",
    "url_rating_75[\"result_75\"] = url_rating_75[0].map(lambda s: s[1])\n",
    "url_rating_75 = pd.DataFrame(url_rating_75[\"result_75\"])\n",
    "url_rating_75 = url_rating_75.set_index(url_rating_75.index.values)\n",
    "\n",
    "url_rating_25 = pd.DataFrame({\"result_25\": url_rating[\"top category\"]})\n",
    "\n",
    "url_rating_merged = pd.merge(url_rating_25,\n",
    "                            url_rating_75,\n",
    "                            left_index=True,\n",
    "                            right_index=True,\n",
    "                            ).dropna()\n",
    "\n",
    "url_rating_crosstab = pd.crosstab(index=url_rating_merged[\"result_25\"],\n",
    "                                 columns=url_rating_merged[\"result_75\"]\n",
    "                                 )\n",
    "\n",
    "url_rating_crosstab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
